{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aef1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from matching import BibEntry, RefEntry, TextCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b83c7",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../../23120260\")  # Adjust to your data directory\n",
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01708ed",
   "metadata": {},
   "source": [
    "## 2. BibTeX Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a56cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibExtractor:\n",
    "    \"\"\"Extract BibTeX entries from LaTeX sources\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_bibitems(tex_content: str) -> List[BibEntry]:\n",
    "        \"\"\"Extract \\\\bibitem entries\"\"\"\n",
    "        entries = []\n",
    "        pattern = r'\\\\bibitem(?:\\[([^\\]]*)\\])?\\{([^}]+)\\}(.*?)(?=\\\\bibitem|\\\\end\\{thebibliography\\}|\\Z)'\n",
    "        \n",
    "        for match in re.finditer(pattern, tex_content, re.DOTALL):\n",
    "            key = match.group(2).strip()\n",
    "            content = match.group(3).strip()\n",
    "            entry = BibExtractor._parse_bibitem_content(key, content)\n",
    "            entries.append(entry)\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    @staticmethod\n",
    "    def _parse_bibitem_content(key: str, content: str) -> BibEntry:\n",
    "        \"\"\"Parse bibitem content into structured entry\"\"\"\n",
    "        # Clean content\n",
    "        content = re.sub(r'\\\\newblock\\s*', ' ', content)\n",
    "        content = re.sub(r'\\{\\\\em\\s+([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\emph\\{([^}]*)\\}', r'\\1', content)\n",
    "        \n",
    "        # Extract arXiv ID\n",
    "        arxiv_id = TextCleaner.extract_arxiv_id(content) or \"\"\n",
    "        \n",
    "        # Extract year\n",
    "        year = TextCleaner.extract_year(content) or \"\"\n",
    "        \n",
    "        # Try to extract title (often in italics or after authors)\n",
    "        title = \"\"\n",
    "        title_match = re.search(r'\\{\\\\em\\s+([^}]+)\\}', content)\n",
    "        if title_match:\n",
    "            title = title_match.group(1)\n",
    "        else:\n",
    "            parts = content.split('.')\n",
    "            if len(parts) > 1:\n",
    "                title = parts[1].strip()\n",
    "        \n",
    "        # Extract authors (before first period or newblock)\n",
    "        authors = []\n",
    "        author_part = content.split('.')[0] if '.' in content else content[:100]\n",
    "        author_matches = re.split(r'\\s+and\\s+|,\\s*(?=[A-Z])', author_part)\n",
    "        for a in author_matches:\n",
    "            a = a.strip()\n",
    "            if a and len(a) > 2 and not a.startswith('\\\\'):\n",
    "                authors.append(a)\n",
    "        \n",
    "        return BibEntry(\n",
    "            key=key,\n",
    "            title=title,\n",
    "            authors=authors[:10],\n",
    "            year=year,\n",
    "            arxiv_id=arxiv_id,\n",
    "            raw_content=content[:500]\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_bib_file(bib_content: str) -> List[BibEntry]:\n",
    "        \"\"\"Extract entries from BibTeX file\"\"\"\n",
    "        entries = []\n",
    "        pattern = r'@(\\w+)\\{([^,]+),\\s*(.*?)\\n\\}'\n",
    "        \n",
    "        for match in re.finditer(pattern, bib_content, re.DOTALL):\n",
    "            entry_type = match.group(1).lower()\n",
    "            key = match.group(2).strip()\n",
    "            fields_str = match.group(3)\n",
    "            \n",
    "            # Parse fields\n",
    "            fields = {}\n",
    "            field_pattern = r'(\\w+)\\s*=\\s*[{\"]([^}\"]*)[\"}]'\n",
    "            for fm in re.finditer(field_pattern, fields_str, re.DOTALL):\n",
    "                fields[fm.group(1).lower()] = fm.group(2).strip()\n",
    "            \n",
    "            # Extract authors\n",
    "            authors = []\n",
    "            if 'author' in fields:\n",
    "                author_list = re.split(r'\\s+and\\s+', fields['author'])\n",
    "                authors = [a.strip() for a in author_list]\n",
    "            \n",
    "            # Extract arXiv ID\n",
    "            arxiv_id = fields.get('eprint', '') or fields.get('arxiv', '')\n",
    "            if arxiv_id:\n",
    "                arxiv_id = arxiv_id.replace('.', '-')\n",
    "            \n",
    "            entries.append(BibEntry(\n",
    "                key=key,\n",
    "                title=fields.get('title', ''),\n",
    "                authors=authors,\n",
    "                year=fields.get('year', ''),\n",
    "                venue=fields.get('journal', fields.get('booktitle', '')),\n",
    "                arxiv_id=arxiv_id,\n",
    "                raw_content=match.group(0)[:500]\n",
    "            ))\n",
    "        \n",
    "        return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d807f3",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_references_json(path: Path) -> Dict[str, RefEntry]:\n",
    "    \"\"\"Load references.json and convert to RefEntry objects\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    entries = {}\n",
    "    for arxiv_id, info in data.items():\n",
    "        entries[arxiv_id] = RefEntry(\n",
    "            arxiv_id=arxiv_id,\n",
    "            title=info.get('paper_title', ''),\n",
    "            authors=info.get('authors', []),\n",
    "            submission_date=info.get('submission_date', ''),\n",
    "            venue=info.get('publication_venue', '')\n",
    "        )\n",
    "    \n",
    "    return entries\n",
    "\n",
    "\n",
    "def extract_bibs_from_publication(pub_path: Path) -> List[BibEntry]:\n",
    "    \"\"\"Extract all BibTeX entries from a publication's LaTeX sources\"\"\"\n",
    "    tex_path = pub_path / 'tex'\n",
    "    all_entries = []\n",
    "    \n",
    "    if not tex_path.exists():\n",
    "        return all_entries\n",
    "    \n",
    "    # Process all version directories\n",
    "    for version_dir in tex_path.iterdir():\n",
    "        if not version_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # Look for .bib files\n",
    "        for bib_file in version_dir.rglob('*.bib'):\n",
    "            try:\n",
    "                content = bib_file.read_text(encoding='utf-8', errors='ignore')\n",
    "                entries = BibExtractor.extract_from_bib_file(content)\n",
    "                all_entries.extend(entries)\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # Look for thebibliography in .tex files\n",
    "        for tex_file in version_dir.rglob('*.tex'):\n",
    "            try:\n",
    "                content = tex_file.read_text(encoding='utf-8', errors='ignore')\n",
    "                if r'\\bibitem' in content:\n",
    "                    entries = BibExtractor.extract_bibitems(content)\n",
    "                    all_entries.extend(entries)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Deduplicate by key\n",
    "    seen_keys = set()\n",
    "    unique_entries = []\n",
    "    for entry in all_entries:\n",
    "        if entry.key not in seen_keys:\n",
    "            seen_keys.add(entry.key)\n",
    "            unique_entries.append(entry)\n",
    "    \n",
    "    return unique_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50674c",
   "metadata": {},
   "source": [
    "## 4. Process Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e27331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of publications\n",
    "publications = sorted([p for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "print(f\"Found {len(publications)} publications\")\n",
    "\n",
    "# Show sample\n",
    "for pub in publications[:5]:\n",
    "    print(f\"  - {pub.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sample publication\n",
    "sample_pub = publications[0]\n",
    "print(f\"Processing: {sample_pub.name}\")\n",
    "\n",
    "# Load references\n",
    "refs = load_references_json(sample_pub / 'references.json')\n",
    "print(f\"  References: {len(refs)}\")\n",
    "\n",
    "# Extract BibTeX entries\n",
    "bibs = extract_bibs_from_publication(sample_pub)\n",
    "print(f\"  BibTeX entries: {len(bibs)}\")\n",
    "\n",
    "# Show sample entries\n",
    "if bibs:\n",
    "    print(\"\\nSample BibTeX entry:\")\n",
    "    print(f\"  Key: {bibs[0].key}\")\n",
    "    print(f\"  Title: {bibs[0].title[:80]}...\" if len(bibs[0].title) > 80 else f\"  Title: {bibs[0].title}\")\n",
    "    print(f\"  Authors: {bibs[0].authors[:3]}\")\n",
    "    print(f\"  Year: {bibs[0].year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b432b2e6",
   "metadata": {},
   "source": [
    "## 5. Extract All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all publications\n",
    "all_data = []\n",
    "\n",
    "for i, pub_path in enumerate(publications):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processing {i}/{len(publications)}...\")\n",
    "    \n",
    "    refs = load_references_json(pub_path / 'references.json')\n",
    "    bibs = extract_bibs_from_publication(pub_path)\n",
    "    \n",
    "    if refs and bibs:\n",
    "        all_data.append({\n",
    "            'pub_id': pub_path.name,\n",
    "            'num_refs': len(refs),\n",
    "            'num_bibs': len(bibs),\n",
    "            'refs': {k: v.to_dict() for k, v in refs.items()},\n",
    "            'bibs': [b.to_dict() for b in bibs]\n",
    "        })\n",
    "\n",
    "print(f\"\\nProcessed {len(all_data)} publications with both refs and bibs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d3da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save extracted data\n",
    "output_file = OUTPUT_DIR / 'extracted_data.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved extracted data to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac916232",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "total_refs = sum(d['num_refs'] for d in all_data)\n",
    "total_bibs = sum(d['num_bibs'] for d in all_data)\n",
    "\n",
    "print(\"=== Data Summary ===\")\n",
    "print(f\"Publications processed: {len(all_data)}\")\n",
    "print(f\"Total reference candidates: {total_refs}\")\n",
    "print(f\"Total BibTeX entries: {total_bibs}\")\n",
    "print(f\"Average refs per publication: {total_refs / len(all_data):.1f}\")\n",
    "print(f\"Average bibs per publication: {total_bibs / len(all_data):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d3d84",
   "metadata": {},
   "source": [
    "## 7. Automatic Labeling (10% of Non-Manual Data)\n",
    "\n",
    "Requirement 2.2.2: Implement automatic matching based on classic tools (regex, string similarity) to generate labels for at least 10% of the remaining non-manual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoLabeler:\n",
    "    \"\"\"\n",
    "    Automatic labeling using classic matching heuristics.\n",
    "    \n",
    "    Strategies:\n",
    "    1. Exact arXiv ID match in bibitem content\n",
    "    2. High title similarity (Jaccard > 0.7)\n",
    "    3. First author + year + partial title match\n",
    "    \"\"\"\n",
    "    \n",
    "    # Manual publications to exclude from auto-labeling\n",
    "    MANUAL_PUBS = {\"2411-00222\", \"2411-00223\", \"2411-00225\", \"2411-00226\", \"2411-00227\"}\n",
    "    \n",
    "    # Auto-labeled publication partitions (1 for test, 1 for valid, rest train)\n",
    "    AUTO_PARTITIONS = {}  # Will be filled dynamically\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard(set1, set2):\n",
    "        if not set1 or not set2:\n",
    "            return 0.0\n",
    "        return len(set1 & set2) / len(set1 | set2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        if not text:\n",
    "            return set()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "        return set(text.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_last_name(author):\n",
    "        if not author:\n",
    "            return \"\"\n",
    "        parts = author.lower().split()\n",
    "        return parts[-1] if parts else \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_arxiv_in_content(content, refs):\n",
    "        \"\"\"Find exact arXiv ID matches in bibitem content.\"\"\"\n",
    "        matches = []\n",
    "        for arxiv_id in refs.keys():\n",
    "            # Try both formats: 2411-00222 and 2411.00222\n",
    "            arxiv_dot = arxiv_id.replace('-', '.')\n",
    "            if arxiv_dot in content or arxiv_id in content:\n",
    "                matches.append((arxiv_id, 1.0, \"arxiv_exact\"))\n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_title_match(bib, refs, threshold=0.7):\n",
    "        \"\"\"Find high title similarity matches.\"\"\"\n",
    "        bib_title_tokens = AutoLabeler.tokenize(bib.get('title', ''))\n",
    "        if not bib_title_tokens:\n",
    "            return []\n",
    "        \n",
    "        matches = []\n",
    "        for arxiv_id, ref in refs.items():\n",
    "            ref_title_tokens = AutoLabeler.tokenize(ref.get('paper_title', ''))\n",
    "            sim = AutoLabeler.jaccard(bib_title_tokens, ref_title_tokens)\n",
    "            if sim >= threshold:\n",
    "                matches.append((arxiv_id, sim, \"title_jaccard\"))\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_author_year_match(bib, refs):\n",
    "        \"\"\"Find matches by first author + year + partial title.\"\"\"\n",
    "        bib_authors = bib.get('authors', [])\n",
    "        bib_year = bib.get('year', '') or TextCleaner.extract_year(bib.get('raw_content', ''))\n",
    "        bib_title_tokens = AutoLabeler.tokenize(bib.get('title', ''))\n",
    "        \n",
    "        if not bib_authors or not bib_year:\n",
    "            return []\n",
    "        \n",
    "        bib_first_author = AutoLabeler.extract_last_name(bib_authors[0])\n",
    "        \n",
    "        matches = []\n",
    "        for arxiv_id, ref in refs.items():\n",
    "            ref_authors = ref.get('authors', [])\n",
    "            ref_year = ref.get('submission_date', '')[:4]\n",
    "            ref_title_tokens = AutoLabeler.tokenize(ref.get('paper_title', ''))\n",
    "            \n",
    "            if not ref_authors:\n",
    "                continue\n",
    "            \n",
    "            ref_first_author = AutoLabeler.extract_last_name(ref_authors[0])\n",
    "            \n",
    "            # Check first author match\n",
    "            if bib_first_author != ref_first_author:\n",
    "                continue\n",
    "            \n",
    "            # Check year match (within 1 year tolerance)\n",
    "            try:\n",
    "                if abs(int(bib_year) - int(ref_year)) > 1:\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            # Check partial title overlap\n",
    "            title_overlap = AutoLabeler.jaccard(bib_title_tokens, ref_title_tokens)\n",
    "            if title_overlap >= 0.3:\n",
    "                confidence = 0.6 + 0.4 * title_overlap\n",
    "                matches.append((arxiv_id, confidence, \"author_year_title\"))\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def auto_label_publication(pub_data, refs_data):\n",
    "        \"\"\"Generate automatic labels for a publication.\"\"\"\n",
    "        labels = {}\n",
    "        \n",
    "        for bib in pub_data['bibs']:\n",
    "            bib_key = bib['key']\n",
    "            raw_content = bib.get('raw_content', '')\n",
    "            \n",
    "            # Strategy 1: Exact arXiv ID in content (highest confidence)\n",
    "            matches = AutoLabeler.find_arxiv_in_content(raw_content, refs_data)\n",
    "            \n",
    "            # Strategy 2: High title similarity\n",
    "            if not matches:\n",
    "                matches = AutoLabeler.find_title_match(bib, refs_data, threshold=0.7)\n",
    "            \n",
    "            # Strategy 3: Author + year + partial title\n",
    "            if not matches:\n",
    "                matches = AutoLabeler.find_author_year_match(bib, refs_data)\n",
    "            \n",
    "            # Take best match if any\n",
    "            if matches:\n",
    "                best_match = max(matches, key=lambda x: x[1])\n",
    "                labels[bib_key] = {\n",
    "                    'arxiv_id': best_match[0],\n",
    "                    'confidence': best_match[1],\n",
    "                    'method': best_match[2]\n",
    "                }\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run auto-labeling on non-manual publications\n",
    "# Requirement: at least 10% of remaining data\n",
    "\n",
    "# Filter out manual publications\n",
    "non_manual_data = [d for d in all_data if d['pub_id'] not in AutoLabeler.MANUAL_PUBS]\n",
    "print(f\"Non-manual publications: {len(non_manual_data)}\")\n",
    "\n",
    "# Calculate 10% target\n",
    "target_count = max(1, int(len(non_manual_data) * 0.10))\n",
    "print(f\"Target for 10% coverage: {target_count} publications\")\n",
    "\n",
    "# Run auto-labeling and collect results\n",
    "auto_labeled_results = []\n",
    "\n",
    "for pub_data in non_manual_data:\n",
    "    refs_data = pub_data['refs']\n",
    "    labels = AutoLabeler.auto_label_publication(pub_data, refs_data)\n",
    "    \n",
    "    if labels:  # Only include if we found at least one label\n",
    "        auto_labeled_results.append({\n",
    "            'pub_id': pub_data['pub_id'],\n",
    "            'labels': labels,\n",
    "            'num_labels': len(labels)\n",
    "        })\n",
    "\n",
    "# Sort by number of labels (prefer publications with more labels)\n",
    "auto_labeled_results.sort(key=lambda x: x['num_labels'], reverse=True)\n",
    "\n",
    "# Take at least 10% (or more if available)\n",
    "selected_auto = auto_labeled_results[:max(target_count, len(auto_labeled_results))]\n",
    "\n",
    "print(f\"\\nAuto-labeling results:\")\n",
    "print(f\"  Publications with auto-labels: {len(auto_labeled_results)}\")\n",
    "print(f\"  Selected for 10% requirement: {len(selected_auto)}\")\n",
    "print(f\"  Coverage: {len(selected_auto) / len(non_manual_data) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign partitions to auto-labeled publications\n",
    "# Requirement: 1 for test, 1 for valid, rest for train\n",
    "\n",
    "AUTO_PARTITIONS = {}\n",
    "for i, result in enumerate(selected_auto):\n",
    "    pub_id = result['pub_id']\n",
    "    if i == 0:\n",
    "        AUTO_PARTITIONS[pub_id] = \"test\"   # First auto-labeled -> test\n",
    "    elif i == 1:\n",
    "        AUTO_PARTITIONS[pub_id] = \"valid\"  # Second auto-labeled -> valid\n",
    "    else:\n",
    "        AUTO_PARTITIONS[pub_id] = \"train\"\n",
    "\n",
    "print(\"Auto-labeled partition assignments:\")\n",
    "for pub_id, partition in list(AUTO_PARTITIONS.items())[:10]:\n",
    "    print(f\"  {pub_id}: {partition}\")\n",
    "if len(AUTO_PARTITIONS) > 10:\n",
    "    print(f\"  ... and {len(AUTO_PARTITIONS) - 10} more (train)\")\n",
    "\n",
    "# Save auto-labels to pred.json files\n",
    "saved_count = 0\n",
    "for result in selected_auto:\n",
    "    pub_id = result['pub_id']\n",
    "    pub_path = DATA_DIR / pub_id\n",
    "    partition = AUTO_PARTITIONS.get(pub_id, \"train\")\n",
    "    \n",
    "    # Convert labels to groundtruth format\n",
    "    groundtruth = {k: v['arxiv_id'] for k, v in result['labels'].items()}\n",
    "    prediction = {k: [] for k in groundtruth.keys()}\n",
    "    \n",
    "    pred_data = {\n",
    "        \"partition\": partition,\n",
    "        \"groundtruth\": groundtruth,\n",
    "        \"prediction\": prediction,\n",
    "        \"label_source\": \"auto\",\n",
    "        \"label_methods\": {k: v['method'] for k, v in result['labels'].items()}\n",
    "    }\n",
    "    \n",
    "    pred_file = pub_path / \"pred.json\"\n",
    "    with open(pred_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_data, f, indent=2, ensure_ascii=False)\n",
    "    saved_count += 1\n",
    "\n",
    "print(f\"\\nSaved {saved_count} auto-labeled pred.json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Combined labeling statistics\n",
    "total_auto_labels = sum(r['num_labels'] for r in selected_auto)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LABELING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nManual Labels:\")\n",
    "print(f\"  Publications: 5\")\n",
    "print(f\"  Test: 2411-00222\")\n",
    "print(f\"  Valid: 2411-00223\")\n",
    "print(f\"  Train: 2411-00225, 2411-00226, 2411-00227\")\n",
    "\n",
    "print(f\"\\nAutomatic Labels:\")\n",
    "print(f\"  Publications: {len(selected_auto)}\")\n",
    "print(f\"  Total label pairs: {total_auto_labels}\")\n",
    "print(f\"  Coverage: {len(selected_auto) / len(non_manual_data) * 100:.1f}% of non-manual data\")\n",
    "print(f\"  Test: {[p for p, part in AUTO_PARTITIONS.items() if part == 'test']}\")\n",
    "print(f\"  Valid: {[p for p, part in AUTO_PARTITIONS.items() if part == 'valid']}\")\n",
    "print(f\"  Train: {sum(1 for part in AUTO_PARTITIONS.values() if part == 'train')} publications\")\n",
    "\n",
    "print(f\"\\nData Split Summary:\")\n",
    "print(f\"  Test set: 1 manual + 1 auto = 2 publications\")\n",
    "print(f\"  Valid set: 1 manual + 1 auto = 2 publications\")\n",
    "print(f\"  Train set: 3 manual + {sum(1 for part in AUTO_PARTITIONS.values() if part == 'train')} auto publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5cb90",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** Continue to `02_feature_engineering.ipynb` to extract matching features."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
