{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51aef1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from matching import BibEntry, RefEntry, TextCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b83c7",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1fb811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: ..\\..\\23120260\n",
      "Output directory: ..\\..\\output\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../../23120260\")  # Adjust to your data directory\n",
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01708ed",
   "metadata": {},
   "source": [
    "## 2. BibTeX Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a56cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibExtractor:\n",
    "    \"\"\"Extract BibTeX entries from LaTeX sources\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_bibitems(tex_content: str) -> List[BibEntry]:\n",
    "        \"\"\"Extract \\\\bibitem entries\"\"\"\n",
    "        entries = []\n",
    "        pattern = r'\\\\bibitem(?:\\[([^\\]]*)\\])?\\{([^}]+)\\}(.*?)(?=\\\\bibitem|\\\\end\\{thebibliography\\}|\\Z)'\n",
    "        \n",
    "        for match in re.finditer(pattern, tex_content, re.DOTALL):\n",
    "            key = match.group(2).strip()\n",
    "            content = match.group(3).strip()\n",
    "            entry = BibExtractor._parse_bibitem_content(key, content)\n",
    "            entries.append(entry)\n",
    "        \n",
    "        return entries\n",
    "    \n",
    "    @staticmethod\n",
    "    def _parse_bibitem_content(key: str, content: str) -> BibEntry:\n",
    "        \"\"\"Parse bibitem content into structured entry\"\"\"\n",
    "        # Clean content\n",
    "        content = re.sub(r'\\\\newblock\\s*', ' ', content)\n",
    "        content = re.sub(r'\\{\\\\em\\s+([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\emph\\{([^}]*)\\}', r'\\1', content)\n",
    "        \n",
    "        # Extract arXiv ID\n",
    "        arxiv_id = TextCleaner.extract_arxiv_id(content) or \"\"\n",
    "        \n",
    "        # Extract year\n",
    "        year = TextCleaner.extract_year(content) or \"\"\n",
    "        \n",
    "        # Try to extract title (often in italics or after authors)\n",
    "        title = \"\"\n",
    "        title_match = re.search(r'\\{\\\\em\\s+([^}]+)\\}', content)\n",
    "        if title_match:\n",
    "            title = title_match.group(1)\n",
    "        else:\n",
    "            parts = content.split('.')\n",
    "            if len(parts) > 1:\n",
    "                title = parts[1].strip()\n",
    "        \n",
    "        # Extract authors (before first period or newblock)\n",
    "        authors = []\n",
    "        author_part = content.split('.')[0] if '.' in content else content[:100]\n",
    "        author_matches = re.split(r'\\s+and\\s+|,\\s*(?=[A-Z])', author_part)\n",
    "        for a in author_matches:\n",
    "            a = a.strip()\n",
    "            if a and len(a) > 2 and not a.startswith('\\\\'):\n",
    "                authors.append(a)\n",
    "        \n",
    "        return BibEntry(\n",
    "            key=key,\n",
    "            title=title,\n",
    "            authors=authors[:10],\n",
    "            year=year,\n",
    "            arxiv_id=arxiv_id,\n",
    "            raw_content=content[:500]\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_bib_file(bib_content: str) -> List[BibEntry]:\n",
    "        \"\"\"Extract entries from BibTeX file\"\"\"\n",
    "        entries = []\n",
    "        pattern = r'@(\\w+)\\{([^,]+),\\s*(.*?)\\n\\}'\n",
    "        \n",
    "        for match in re.finditer(pattern, bib_content, re.DOTALL):\n",
    "            entry_type = match.group(1).lower()\n",
    "            key = match.group(2).strip()\n",
    "            fields_str = match.group(3)\n",
    "            \n",
    "            # Parse fields\n",
    "            fields = {}\n",
    "            field_pattern = r'(\\w+)\\s*=\\s*[{\"]([^}\"]*)[\"}]'\n",
    "            for fm in re.finditer(field_pattern, fields_str, re.DOTALL):\n",
    "                fields[fm.group(1).lower()] = fm.group(2).strip()\n",
    "            \n",
    "            # Extract authors\n",
    "            authors = []\n",
    "            if 'author' in fields:\n",
    "                author_list = re.split(r'\\s+and\\s+', fields['author'])\n",
    "                authors = [a.strip() for a in author_list]\n",
    "            \n",
    "            # Extract arXiv ID\n",
    "            arxiv_id = fields.get('eprint', '') or fields.get('arxiv', '')\n",
    "            if arxiv_id:\n",
    "                arxiv_id = arxiv_id.replace('.', '-')\n",
    "            \n",
    "            entries.append(BibEntry(\n",
    "                key=key,\n",
    "                title=fields.get('title', ''),\n",
    "                authors=authors,\n",
    "                year=fields.get('year', ''),\n",
    "                venue=fields.get('journal', fields.get('booktitle', '')),\n",
    "                arxiv_id=arxiv_id,\n",
    "                raw_content=match.group(0)[:500]\n",
    "            ))\n",
    "        \n",
    "        return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d807f3",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ec7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_references_json(path: Path) -> Dict[str, RefEntry]:\n",
    "    \"\"\"Load references.json and convert to RefEntry objects\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    entries = {}\n",
    "    for arxiv_id, info in data.items():\n",
    "        entries[arxiv_id] = RefEntry(\n",
    "            arxiv_id=arxiv_id,\n",
    "            title=info.get('paper_title', ''),\n",
    "            authors=info.get('authors', []),\n",
    "            submission_date=info.get('submission_date', ''),\n",
    "            venue=info.get('publication_venue', '')\n",
    "        )\n",
    "    \n",
    "    return entries\n",
    "\n",
    "\n",
    "def extract_bibs_from_publication(pub_path: Path) -> List[BibEntry]:\n",
    "    \"\"\"Extract all BibTeX entries from a publication's LaTeX sources\"\"\"\n",
    "    tex_path = pub_path / 'tex'\n",
    "    all_entries = []\n",
    "    \n",
    "    if not tex_path.exists():\n",
    "        return all_entries\n",
    "    \n",
    "    # Process all version directories\n",
    "    for version_dir in tex_path.iterdir():\n",
    "        if not version_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # Look for .bib files\n",
    "        for bib_file in version_dir.rglob('*.bib'):\n",
    "            try:\n",
    "                content = bib_file.read_text(encoding='utf-8', errors='ignore')\n",
    "                entries = BibExtractor.extract_from_bib_file(content)\n",
    "                all_entries.extend(entries)\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # Look for thebibliography in .tex files\n",
    "        for tex_file in version_dir.rglob('*.tex'):\n",
    "            try:\n",
    "                content = tex_file.read_text(encoding='utf-8', errors='ignore')\n",
    "                if r'\\bibitem' in content:\n",
    "                    entries = BibExtractor.extract_bibitems(content)\n",
    "                    all_entries.extend(entries)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Deduplicate by key\n",
    "    seen_keys = set()\n",
    "    unique_entries = []\n",
    "    for entry in all_entries:\n",
    "        if entry.key not in seen_keys:\n",
    "            seen_keys.add(entry.key)\n",
    "            unique_entries.append(entry)\n",
    "    \n",
    "    return unique_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50674c",
   "metadata": {},
   "source": [
    "## 4. Process Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04e27331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 total publications\n",
      "  Manual labeled: 5\n",
      "  Non-manual: 4995\n",
      "\n",
      "Selected 1505 publications:\n",
      "  - 5 manual labeled\n",
      "  - 1500 random sampled\n",
      "\n",
      "First 5 publications:\n",
      "  - 2411-00222 [MANUAL]\n",
      "  - 2411-00223 [MANUAL]\n",
      "  - 2411-00225 [MANUAL]\n",
      "  - 2411-00226 [MANUAL]\n",
      "  - 2411-00227 [MANUAL]\n"
     ]
    }
   ],
   "source": [
    "# Get list of publications\n",
    "import random\n",
    "\n",
    "all_publications = sorted([p for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "print(f\"Found {len(all_publications)} total publications\")\n",
    "\n",
    "# Manual labeled publications (must include)\n",
    "MANUAL_PUBS = {\"2411-00222\", \"2411-00223\", \"2411-00225\", \"2411-00226\", \"2411-00227\"}\n",
    "\n",
    "# Separate manual and non-manual publications\n",
    "manual_pubs = [p for p in all_publications if p.name in MANUAL_PUBS]\n",
    "non_manual_pubs = [p for p in all_publications if p.name not in MANUAL_PUBS]\n",
    "\n",
    "print(f\"  Manual labeled: {len(manual_pubs)}\")\n",
    "print(f\"  Non-manual: {len(non_manual_pubs)}\")\n",
    "\n",
    "# Sample 500 random non-manual publications\n",
    "random.seed(42)  # For reproducibility\n",
    "sample_size = 1500\n",
    "sampled_non_manual = random.sample(non_manual_pubs, min(sample_size, len(non_manual_pubs)))\n",
    "\n",
    "# Combine: all manual + 500 sampled\n",
    "publications = manual_pubs + sampled_non_manual\n",
    "publications = sorted(publications, key=lambda p: p.name)\n",
    "\n",
    "print(f\"\\nSelected {len(publications)} publications:\")\n",
    "print(f\"  - {len(manual_pubs)} manual labeled\")\n",
    "print(f\"  - {len(sampled_non_manual)} random sampled\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nFirst 5 publications:\")\n",
    "for pub in publications[:5]:\n",
    "    marker = \" [MANUAL]\" if pub.name in MANUAL_PUBS else \"\"\n",
    "    print(f\"  - {pub.name}{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a62e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 2411-00222\n",
      "  References: 16\n",
      "  BibTeX entries: 31\n",
      "\n",
      "Sample BibTeX entry:\n",
      "  Key: ganjidoost2024protectingfeedforwardnetworksadversarial\n",
      "  Title: Protecting Feed-Forward Networks from Adversarial Attacks Using Predictive Codin...\n",
      "  Authors: ['Ehsan Ganjidoost', 'Jeff Orchard']\n",
      "  Year: 2024\n"
     ]
    }
   ],
   "source": [
    "# Process a sample publication\n",
    "sample_pub = publications[0]\n",
    "print(f\"Processing: {sample_pub.name}\")\n",
    "\n",
    "# Load references\n",
    "refs = load_references_json(sample_pub / 'references.json')\n",
    "print(f\"  References: {len(refs)}\")\n",
    "\n",
    "# Extract BibTeX entries\n",
    "bibs = extract_bibs_from_publication(sample_pub)\n",
    "print(f\"  BibTeX entries: {len(bibs)}\")\n",
    "\n",
    "# Show sample entries\n",
    "if bibs:\n",
    "    print(\"\\nSample BibTeX entry:\")\n",
    "    print(f\"  Key: {bibs[0].key}\")\n",
    "    print(f\"  Title: {bibs[0].title[:80]}...\" if len(bibs[0].title) > 80 else f\"  Title: {bibs[0].title}\")\n",
    "    print(f\"  Authors: {bibs[0].authors[:3]}\")\n",
    "    print(f\"  Year: {bibs[0].year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b432b2e6",
   "metadata": {},
   "source": [
    "## 5. Extract All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d735f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with 16 threads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing publications: 100%|██████████| 1505/1505 [00:57<00:00, 25.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VALIDATION SUMMARY\n",
      "============================================================\n",
      "Total publications scanned: 1505\n",
      "Valid publications: 893\n",
      "\n",
      "Skipped publications:\n",
      "  - No references.json: 179\n",
      "  - Empty references.json: 110\n",
      "  - No tex folder: 136\n",
      "  - No BibTeX entries found: 187\n",
      "  - Total skipped: 612\n",
      "\n",
      "============================================================\n",
      "CANDIDATE FILTERING SUMMARY\n",
      "============================================================\n",
      "Total refs before filtering: 20767\n",
      "Total refs after filtering: 20408\n",
      "Refs removed: 359 (1.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all publications with detailed validation and candidate filtering\n",
    "# Using ThreadPoolExecutor for parallel I/O operations\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "skipped = {\n",
    "    'no_refs_file': [],\n",
    "    'empty_refs': [],\n",
    "    'no_bibs': [],\n",
    "    'no_tex_folder': []\n",
    "}\n",
    "\n",
    "# Stats for filtering\n",
    "total_refs_before = 0\n",
    "total_refs_after = 0\n",
    "\n",
    "def filter_refs_for_bibs(bibs: List[BibEntry], refs: Dict[str, RefEntry], max_title_diff: int = 40) -> Dict[str, RefEntry]:\n",
    "    \"\"\"\n",
    "    Pre-filter refs to only keep candidates that could potentially match any bib.\n",
    "    A ref is kept if its title length is within max_title_diff of at least one bib title.\n",
    "    \"\"\"\n",
    "    filtered_refs = {}\n",
    "    \n",
    "    # Get all bib title lengths\n",
    "    bib_title_lengths = []\n",
    "    for bib in bibs:\n",
    "        title = bib.title.replace('\\n', ' ').strip() if bib.title else ''\n",
    "        bib_title_lengths.append(len(title))\n",
    "    \n",
    "    # Keep refs where title length is close to at least one bib\n",
    "    for arxiv_id, ref in refs.items():\n",
    "        ref_title = ref.title.replace('\\n', ' ').strip() if ref.title else ''\n",
    "        ref_len = len(ref_title)\n",
    "        \n",
    "        # Check if this ref could match any bib based on title length\n",
    "        for bib_len in bib_title_lengths:\n",
    "            if abs(ref_len - bib_len) <= max_title_diff:\n",
    "                filtered_refs[arxiv_id] = ref\n",
    "                break\n",
    "    \n",
    "    return filtered_refs\n",
    "\n",
    "def process_publication(pub_path: Path) -> dict:\n",
    "    \"\"\"Process a single publication - designed for parallel execution\"\"\"\n",
    "    result = {'status': 'skipped', 'reason': None, 'data': None, 'stats': {}}\n",
    "    \n",
    "    try:\n",
    "        # Check references.json\n",
    "        refs_path = pub_path / 'references.json'\n",
    "        if not refs_path.exists():\n",
    "            result['reason'] = 'no_refs_file'\n",
    "            result['pub_id'] = pub_path.name\n",
    "            return result\n",
    "        \n",
    "        refs = load_references_json(refs_path)\n",
    "        if not refs:\n",
    "            result['reason'] = 'empty_refs'\n",
    "            result['pub_id'] = pub_path.name\n",
    "            return result\n",
    "        \n",
    "        # Check tex folder exists\n",
    "        tex_path = pub_path / 'tex'\n",
    "        if not tex_path.exists():\n",
    "            result['reason'] = 'no_tex_folder'\n",
    "            result['pub_id'] = pub_path.name\n",
    "            return result\n",
    "        \n",
    "        # Extract bibs\n",
    "        bibs = extract_bibs_from_publication(pub_path)\n",
    "        if not bibs:\n",
    "            result['reason'] = 'no_bibs'\n",
    "            result['pub_id'] = pub_path.name\n",
    "            return result\n",
    "        \n",
    "        # Track stats\n",
    "        refs_before = len(refs)\n",
    "        \n",
    "        # Filter refs based on title length compatibility with bibs\n",
    "        filtered_refs = filter_refs_for_bibs(bibs, refs, max_title_diff=40)\n",
    "        refs_after = len(filtered_refs)\n",
    "        \n",
    "        # Valid publication (only save filtered refs)\n",
    "        result['status'] = 'success'\n",
    "        result['stats'] = {'refs_before': refs_before, 'refs_after': refs_after}\n",
    "        result['data'] = {\n",
    "            'pub_id': pub_path.name,\n",
    "            'num_refs': refs_after,\n",
    "            'num_bibs': len(bibs),\n",
    "            'refs': {k: v.to_dict() for k, v in filtered_refs.items()},\n",
    "            'bibs': [b.to_dict() for b in bibs]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['status'] = 'error'\n",
    "        result['reason'] = 'error'\n",
    "        result['pub_id'] = pub_path.name\n",
    "        result['error'] = str(e)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Process with ThreadPoolExecutor (better for I/O-bound operations)\n",
    "num_workers = min(multiprocessing.cpu_count() * 2, 16)  # More threads for I/O\n",
    "print(f\"Processing with {num_workers} threads...\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    futures = [executor.submit(process_publication, pub_path) for pub_path in publications]\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(publications), desc=\"Processing publications\"):\n",
    "        result = future.result()\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            all_data.append(result['data'])\n",
    "            total_refs_before += result['stats']['refs_before']\n",
    "            total_refs_after += result['stats']['refs_after']\n",
    "        elif result['status'] == 'skipped':\n",
    "            skipped[result['reason']].append(result['pub_id'])\n",
    "        elif result['status'] == 'error':\n",
    "            print(f\"\\nError processing {result['pub_id']}: {result.get('error', 'Unknown')}\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total publications scanned: {len(publications)}\")\n",
    "print(f\"Valid publications: {len(all_data)}\")\n",
    "print(f\"\\nSkipped publications:\")\n",
    "print(f\"  - No references.json: {len(skipped['no_refs_file'])}\")\n",
    "print(f\"  - Empty references.json: {len(skipped['empty_refs'])}\")\n",
    "print(f\"  - No tex folder: {len(skipped['no_tex_folder'])}\")\n",
    "print(f\"  - No BibTeX entries found: {len(skipped['no_bibs'])}\")\n",
    "print(f\"  - Total skipped: {sum(len(v) for v in skipped.values())}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CANDIDATE FILTERING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total refs before filtering: {total_refs_before}\")\n",
    "print(f\"Total refs after filtering: {total_refs_after}\")\n",
    "if total_refs_before > 0:\n",
    "    print(f\"Refs removed: {total_refs_before - total_refs_after} ({(total_refs_before - total_refs_after) / total_refs_before * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c7d3da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted data to: ..\\..\\output\\extracted_data.json\n"
     ]
    }
   ],
   "source": [
    "# Save extracted data\n",
    "output_file = OUTPUT_DIR / 'extracted_data.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved extracted data to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac916232",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7faa834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Summary ===\n",
      "Publications processed: 893\n",
      "Total reference candidates: 20408\n",
      "Total BibTeX entries: 422393\n",
      "Average refs per publication: 22.9\n",
      "Average bibs per publication: 473.0\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics\n",
    "total_refs = sum(d['num_refs'] for d in all_data)\n",
    "total_bibs = sum(d['num_bibs'] for d in all_data)\n",
    "\n",
    "print(\"=== Data Summary ===\")\n",
    "print(f\"Publications processed: {len(all_data)}\")\n",
    "print(f\"Total reference candidates: {total_refs}\")\n",
    "print(f\"Total BibTeX entries: {total_bibs}\")\n",
    "print(f\"Average refs per publication: {total_refs / len(all_data):.1f}\")\n",
    "print(f\"Average bibs per publication: {total_bibs / len(all_data):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d3d84",
   "metadata": {},
   "source": [
    "## 7. Automatic Labeling (10% of Non-Manual Data)\n",
    "\n",
    "Requirement 2.2.2: Implement automatic matching based on classic tools (regex, string similarity) to generate labels for at least 10% of the remaining non-manual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9169b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoLabeler:\n",
    "    \"\"\"\n",
    "    Automatic labeling using classic matching heuristics.\n",
    "    \n",
    "    Strategies:\n",
    "    1. Exact arXiv ID match in bibitem content\n",
    "    2. High title similarity (Jaccard > 0.7)\n",
    "    3. First author + year + partial title match\n",
    "    \"\"\"\n",
    "    \n",
    "    # Auto-labeled publication partitions (1 for test, 1 for valid, rest train)\n",
    "    AUTO_PARTITIONS = {}  # Will be filled dynamically\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard(set1, set2):\n",
    "        if not set1 or not set2:\n",
    "            return 0.0\n",
    "        return len(set1 & set2) / len(set1 | set2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        if not text:\n",
    "            return set()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "        return set(text.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_last_name(author):\n",
    "        if not author:\n",
    "            return \"\"\n",
    "        parts = author.lower().split()\n",
    "        return parts[-1] if parts else \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_arxiv_in_content(content, refs):\n",
    "        \"\"\"Find exact arXiv ID matches in bibitem content.\"\"\"\n",
    "        matches = []\n",
    "        for arxiv_id in refs.keys():\n",
    "            # Try both formats: 2411-00222 and 2411.00222\n",
    "            arxiv_dot = arxiv_id.replace('-', '.')\n",
    "            if arxiv_dot in content or arxiv_id in content:\n",
    "                matches.append((arxiv_id, 1.0, \"arxiv_exact\"))\n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_title_match(bib, refs, threshold=0.7):\n",
    "        \"\"\"Find high title similarity matches.\"\"\"\n",
    "        bib_title_tokens = AutoLabeler.tokenize(bib.get('title', ''))\n",
    "        if not bib_title_tokens:\n",
    "            return []\n",
    "        \n",
    "        matches = []\n",
    "        for arxiv_id, ref in refs.items():\n",
    "            ref_title_tokens = AutoLabeler.tokenize(ref.get('paper_title', ''))\n",
    "            sim = AutoLabeler.jaccard(bib_title_tokens, ref_title_tokens)\n",
    "            if sim >= threshold:\n",
    "                matches.append((arxiv_id, sim, \"title_jaccard\"))\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_author_year_match(bib, refs):\n",
    "        \"\"\"Find matches by first author + year + partial title.\"\"\"\n",
    "        bib_authors = bib.get('authors', [])\n",
    "        bib_year = bib.get('year', '') or TextCleaner.extract_year(bib.get('raw_content', ''))\n",
    "        bib_title_tokens = AutoLabeler.tokenize(bib.get('title', ''))\n",
    "        \n",
    "        if not bib_authors or not bib_year:\n",
    "            return []\n",
    "        \n",
    "        bib_first_author = AutoLabeler.extract_last_name(bib_authors[0])\n",
    "        \n",
    "        matches = []\n",
    "        for arxiv_id, ref in refs.items():\n",
    "            ref_authors = ref.get('authors', [])\n",
    "            ref_year = ref.get('submission_date', '')[:4]\n",
    "            ref_title_tokens = AutoLabeler.tokenize(ref.get('paper_title', ''))\n",
    "            \n",
    "            if not ref_authors:\n",
    "                continue\n",
    "            \n",
    "            ref_first_author = AutoLabeler.extract_last_name(ref_authors[0])\n",
    "            \n",
    "            # Check first author match\n",
    "            if bib_first_author != ref_first_author:\n",
    "                continue\n",
    "            \n",
    "            # Check year match (within 1 year tolerance)\n",
    "            try:\n",
    "                if abs(int(bib_year) - int(ref_year)) > 1:\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            # Check partial title overlap\n",
    "            title_overlap = AutoLabeler.jaccard(bib_title_tokens, ref_title_tokens)\n",
    "            if title_overlap >= 0.3:\n",
    "                confidence = 0.6 + 0.4 * title_overlap\n",
    "                matches.append((arxiv_id, confidence, \"author_year_title\"))\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def auto_label_publication(pub_data, refs_data):\n",
    "        \"\"\"Generate automatic labels for a publication.\"\"\"\n",
    "        labels = {}\n",
    "        \n",
    "        for bib in pub_data['bibs']:\n",
    "            bib_key = bib['key']\n",
    "            raw_content = bib.get('raw_content', '')\n",
    "            \n",
    "            # Strategy 1: Exact arXiv ID in content (highest confidence)\n",
    "            matches = AutoLabeler.find_arxiv_in_content(raw_content, refs_data)\n",
    "            \n",
    "            # Strategy 2: High title similarity\n",
    "            if not matches:\n",
    "                matches = AutoLabeler.find_title_match(bib, refs_data, threshold=0.7)\n",
    "            \n",
    "            # Strategy 3: Author + year + partial title\n",
    "            if not matches:\n",
    "                matches = AutoLabeler.find_author_year_match(bib, refs_data)\n",
    "            \n",
    "            # Take best match if any\n",
    "            if matches:\n",
    "                best_match = max(matches, key=lambda x: x[1])\n",
    "                labels[bib_key] = {\n",
    "                    'arxiv_id': best_match[0],\n",
    "                    'confidence': best_match[1],\n",
    "                    'method': best_match[2]\n",
    "                }\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "142f4d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-manual publications to auto-label: 888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-labeling: 100%|██████████| 888/888 [00:12<00:00, 73.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Auto-labeling results:\n",
      "  Publications with auto-labels: 664\n",
      "  Publications without labels: 224\n",
      "  Coverage: 74.8% of sampled data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run auto-labeling on ALL non-manual publications (all 500 sampled)\n",
    "# Since we only extracted 500 + 5 manual, we auto-label all 500 non-manual\n",
    "\n",
    "# Filter out manual publications\n",
    "non_manual_data = [d for d in all_data if d['pub_id'] not in MANUAL_PUBS]\n",
    "print(f\"Non-manual publications to auto-label: {len(non_manual_data)}\")\n",
    "\n",
    "# Run auto-labeling on ALL of them (not just 10%)\n",
    "auto_labeled_results = []\n",
    "\n",
    "for pub_data in tqdm(non_manual_data, desc=\"Auto-labeling\"):\n",
    "    refs_data = pub_data['refs']\n",
    "    labels = AutoLabeler.auto_label_publication(pub_data, refs_data)\n",
    "    \n",
    "    if labels:  # Only include if we found at least one label\n",
    "        auto_labeled_results.append({\n",
    "            'pub_id': pub_data['pub_id'],\n",
    "            'labels': labels,\n",
    "            'num_labels': len(labels)\n",
    "        })\n",
    "\n",
    "# Use ALL auto-labeled results (no filtering by target_count)\n",
    "selected_auto = auto_labeled_results\n",
    "\n",
    "print(f\"\\nAuto-labeling results:\")\n",
    "print(f\"  Publications with auto-labels: {len(auto_labeled_results)}\")\n",
    "print(f\"  Publications without labels: {len(non_manual_data) - len(auto_labeled_results)}\")\n",
    "print(f\"  Coverage: {len(selected_auto) / len(non_manual_data) * 100:.1f}% of sampled data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca36b4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-labeled partition assignments:\n",
      "  2411-00252: test\n",
      "  2411-00285: valid\n",
      "  2411-00256: train\n",
      "  2411-00238: train\n",
      "  2411-00248: train\n",
      "  2411-00310: train\n",
      "  2411-00265: train\n",
      "  2411-00230: train\n",
      "  2411-00287: train\n",
      "  2411-00314: train\n",
      "  ... and 654 more (train)\n",
      "\n",
      "Saved 664 auto-labeled pred.json files\n"
     ]
    }
   ],
   "source": [
    "# Assign partitions to auto-labeled publications\n",
    "# Requirement: 1 for test, 1 for valid, rest for train\n",
    "\n",
    "AUTO_PARTITIONS = {}\n",
    "for i, result in enumerate(selected_auto):\n",
    "    pub_id = result['pub_id']\n",
    "    if i == 0:\n",
    "        AUTO_PARTITIONS[pub_id] = \"test\"   # First auto-labeled -> test\n",
    "    elif i == 1:\n",
    "        AUTO_PARTITIONS[pub_id] = \"valid\"  # Second auto-labeled -> valid\n",
    "    else:\n",
    "        AUTO_PARTITIONS[pub_id] = \"train\"\n",
    "\n",
    "print(\"Auto-labeled partition assignments:\")\n",
    "for pub_id, partition in list(AUTO_PARTITIONS.items())[:10]:\n",
    "    print(f\"  {pub_id}: {partition}\")\n",
    "if len(AUTO_PARTITIONS) > 10:\n",
    "    print(f\"  ... and {len(AUTO_PARTITIONS) - 10} more (train)\")\n",
    "\n",
    "# Save auto-labels to pred.json files\n",
    "saved_count = 0\n",
    "for result in selected_auto:\n",
    "    pub_id = result['pub_id']\n",
    "    pub_path = DATA_DIR / pub_id\n",
    "    partition = AUTO_PARTITIONS.get(pub_id, \"train\")\n",
    "    \n",
    "    # Convert labels to groundtruth format\n",
    "    groundtruth = {k: v['arxiv_id'] for k, v in result['labels'].items()}\n",
    "    prediction = {k: [] for k in groundtruth.keys()}\n",
    "    \n",
    "    pred_data = {\n",
    "        \"partition\": partition,\n",
    "        \"groundtruth\": groundtruth,\n",
    "        \"prediction\": prediction,\n",
    "        \"label_source\": \"auto\",\n",
    "        \"label_methods\": {k: v['method'] for k, v in result['labels'].items()}\n",
    "    }\n",
    "    \n",
    "    pred_file = pub_path / \"pred.json\"\n",
    "    with open(pred_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_data, f, indent=2, ensure_ascii=False)\n",
    "    saved_count += 1\n",
    "\n",
    "print(f\"\\nSaved {saved_count} auto-labeled pred.json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "713b9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LABELING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Manual Labels:\n",
      "  Publications: 5\n",
      "  Test: 2411-00222\n",
      "  Valid: 2411-00223\n",
      "  Train: 2411-00225, 2411-00226, 2411-00227\n",
      "\n",
      "Automatic Labels (from 888 sampled papers):\n",
      "  Publications with labels: 664\n",
      "  Total label pairs: 5891\n",
      "  Coverage: 74.8%\n",
      "  Test: ['2411-00252']\n",
      "  Valid: ['2411-00285']\n",
      "  Train: 662 publications\n",
      "\n",
      "Data Split Summary:\n",
      "  Test set: 1 manual + 1 auto = 2 publications\n",
      "  Valid set: 1 manual + 1 auto = 2 publications\n",
      "  Train set: 3 manual + 662 auto publications\n",
      "\n",
      "Total extracted publications: 893 (5 manual + 888 sampled)\n"
     ]
    }
   ],
   "source": [
    "# Summary: Combined labeling statistics\n",
    "total_auto_labels = sum(r['num_labels'] for r in selected_auto)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LABELING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nManual Labels:\")\n",
    "print(f\"  Publications: {len(MANUAL_PUBS)}\")\n",
    "print(f\"  Test: 2411-00222\")\n",
    "print(f\"  Valid: 2411-00223\")\n",
    "print(f\"  Train: 2411-00225, 2411-00226, 2411-00227\")\n",
    "\n",
    "print(f\"\\nAutomatic Labels (from {len(non_manual_data)} sampled papers):\")\n",
    "print(f\"  Publications with labels: {len(selected_auto)}\")\n",
    "print(f\"  Total label pairs: {total_auto_labels}\")\n",
    "print(f\"  Coverage: {len(selected_auto) / len(non_manual_data) * 100:.1f}%\")\n",
    "print(f\"  Test: {[p for p, part in AUTO_PARTITIONS.items() if part == 'test']}\")\n",
    "print(f\"  Valid: {[p for p, part in AUTO_PARTITIONS.items() if part == 'valid']}\")\n",
    "print(f\"  Train: {sum(1 for part in AUTO_PARTITIONS.values() if part == 'train')} publications\")\n",
    "\n",
    "print(f\"\\nData Split Summary:\")\n",
    "print(f\"  Test set: 1 manual + 1 auto = 2 publications\")\n",
    "print(f\"  Valid set: 1 manual + 1 auto = 2 publications\") \n",
    "print(f\"  Train set: 3 manual + {sum(1 for part in AUTO_PARTITIONS.values() if part == 'train')} auto publications\")\n",
    "print(f\"\\nTotal extracted publications: {len(all_data)} (5 manual + {len(non_manual_data)} sampled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5cb90",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** Continue to `02_feature_engineering.ipynb` to extract matching features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
