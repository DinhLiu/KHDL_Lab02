{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51aef1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from matching import BibEntry, RefEntry, TextCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b83c7",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fb811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: ..\\..\\23120260, Output: ..\\..\\output\n",
      "Manual labeled pubs: {'2411-00225', '2411-00222', '2411-00226', '2411-00223', '2411-00227'}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "from matching.common import MANUAL_PARTITIONS, DATA_DIR as COMMON_DATA_DIR, OUTPUT_DIR as COMMON_OUTPUT_DIR\n",
    "\n",
    "DATA_DIR = Path(\"../../23120260\")\n",
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Manual labeled publications (from common.py config)\n",
    "MANUAL_PUBS = set(MANUAL_PARTITIONS.keys())\n",
    "\n",
    "print(f\"Data: {DATA_DIR}, Output: {OUTPUT_DIR}\")\n",
    "print(f\"Manual labeled pubs: {MANUAL_PUBS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01708ed",
   "metadata": {},
   "source": [
    "## 2. BibTeX Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a56cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibExtractor:\n",
    "    \"\"\"Extract BibTeX entries from LaTeX sources\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_bibitems(tex_content: str) -> List[BibEntry]:\n",
    "        entries = []\n",
    "        pattern = r'\\\\bibitem(?:\\[([^\\]]*)\\])?\\{([^}]+)\\}(.*?)(?=\\\\bibitem|\\\\end\\{thebibliography\\}|\\Z)'\n",
    "        \n",
    "        for match in re.finditer(pattern, tex_content, re.DOTALL):\n",
    "            key = match.group(2).strip()\n",
    "            content = match.group(3).strip()\n",
    "            entries.append(BibExtractor._parse_bibitem_content(key, content))\n",
    "        return entries\n",
    "    \n",
    "    @staticmethod\n",
    "    def _parse_bibitem_content(key: str, content: str) -> BibEntry:\n",
    "        content = re.sub(r'\\\\newblock\\s*', ' ', content)\n",
    "        content = re.sub(r'\\{\\\\em\\s+([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\emph\\{([^}]*)\\}', r'\\1', content)\n",
    "        \n",
    "        arxiv_id = TextCleaner.extract_arxiv_id(content) or \"\"\n",
    "        year = TextCleaner.extract_year(content) or \"\"\n",
    "        \n",
    "        title = \"\"\n",
    "        title_match = re.search(r'\\{\\\\em\\s+([^}]+)\\}', content)\n",
    "        if title_match:\n",
    "            title = title_match.group(1)\n",
    "        elif '.' in content:\n",
    "            title = content.split('.')[1].strip()\n",
    "        \n",
    "        authors = []\n",
    "        author_part = content.split('.')[0] if '.' in content else content[:100]\n",
    "        for a in re.split(r'\\s+and\\s+|,\\s*(?=[A-Z])', author_part):\n",
    "            a = a.strip()\n",
    "            if a and len(a) > 2 and not a.startswith('\\\\'):\n",
    "                authors.append(a)\n",
    "        \n",
    "        return BibEntry(key=key, title=title, authors=authors[:10], year=year, \n",
    "                        arxiv_id=arxiv_id, raw_content=content[:500])\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_bib_file(bib_content: str) -> List[BibEntry]:\n",
    "        entries = []\n",
    "        pattern = r'@(\\w+)\\{([^,]+),\\s*(.*?)\\n\\}'\n",
    "        \n",
    "        for match in re.finditer(pattern, bib_content, re.DOTALL):\n",
    "            key = match.group(2).strip()\n",
    "            fields_str = match.group(3)\n",
    "            \n",
    "            fields = {}\n",
    "            for fm in re.finditer(r'(\\w+)\\s*=\\s*[{\"]([^}\"]*)[\"}]', fields_str, re.DOTALL):\n",
    "                fields[fm.group(1).lower()] = fm.group(2).strip()\n",
    "            \n",
    "            authors = re.split(r'\\s+and\\s+', fields.get('author', '')) if 'author' in fields else []\n",
    "            arxiv_id = fields.get('eprint', '') or fields.get('arxiv', '')\n",
    "            \n",
    "            entries.append(BibEntry(\n",
    "                key=key, title=fields.get('title', ''), authors=[a.strip() for a in authors],\n",
    "                year=fields.get('year', ''), venue=fields.get('journal', fields.get('booktitle', '')),\n",
    "                arxiv_id=arxiv_id.replace('.', '-'), raw_content=match.group(0)[:500]\n",
    "            ))\n",
    "        return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d807f3",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ec7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_references_json(path: Path) -> Dict[str, RefEntry]:\n",
    "    \"\"\"Load references.json into RefEntry objects\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return {arxiv_id: RefEntry(arxiv_id=arxiv_id, title=info.get('paper_title', ''),\n",
    "                               authors=info.get('authors', []), \n",
    "                               submission_date=info.get('submission_date', ''),\n",
    "                               venue=info.get('publication_venue', ''))\n",
    "            for arxiv_id, info in data.items()}\n",
    "\n",
    "\n",
    "def extract_cited_keys(tex_content: str) -> set:\n",
    "    \"\"\"Extract all citation keys from \\\\cite{...} commands in tex content\"\"\"\n",
    "    cited_keys = set()\n",
    "    # Match \\cite, \\citep, \\citet, \\citeauthor, \\citeyear, etc.\n",
    "    cite_pattern = r'\\\\cite[a-z]*\\{([^}]+)\\}'\n",
    "    for match in re.finditer(cite_pattern, tex_content):\n",
    "        # Handle multiple keys: \\cite{key1, key2, key3}\n",
    "        keys = match.group(1).split(',')\n",
    "        for key in keys:\n",
    "            key = key.strip()\n",
    "            if key:\n",
    "                cited_keys.add(key)\n",
    "    return cited_keys\n",
    "\n",
    "\n",
    "def extract_bibs_from_publication(pub_path: Path) -> List[BibEntry]:\n",
    "    \"\"\"Extract BibTeX entries that are actually cited in the paper\"\"\"\n",
    "    tex_path = pub_path / 'tex'\n",
    "    if not tex_path.exists():\n",
    "        return []\n",
    "    \n",
    "    # Step 1: Collect all cited keys from .tex files\n",
    "    all_cited_keys = set()\n",
    "    all_tex_content = \"\"\n",
    "    \n",
    "    for version_dir in tex_path.iterdir():\n",
    "        if not version_dir.is_dir():\n",
    "            continue\n",
    "        for tex_file in version_dir.rglob('*.tex'):\n",
    "            try:\n",
    "                content = tex_file.read_text(encoding='utf-8', errors='ignore')\n",
    "                all_tex_content += content + \"\\n\"\n",
    "                all_cited_keys.update(extract_cited_keys(content))\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Step 2: Extract bibitems (always keep - they are explicitly written)\n",
    "    all_entries = []\n",
    "    if r'\\bibitem' in all_tex_content:\n",
    "        all_entries.extend(BibExtractor.extract_bibitems(all_tex_content))\n",
    "    \n",
    "    # Step 3: Extract from .bib files, but ONLY keep cited entries\n",
    "    for version_dir in tex_path.iterdir():\n",
    "        if not version_dir.is_dir():\n",
    "            continue\n",
    "        for bib_file in version_dir.rglob('*.bib'):\n",
    "            try:\n",
    "                content = bib_file.read_text(encoding='utf-8', errors='ignore')\n",
    "                bib_entries = BibExtractor.extract_from_bib_file(content)\n",
    "                # Filter: only keep entries that are actually cited\n",
    "                for entry in bib_entries:\n",
    "                    if entry.key in all_cited_keys:\n",
    "                        all_entries.append(entry)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Deduplicate by key\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for e in all_entries:\n",
    "        if e.key not in seen:\n",
    "            seen.add(e.key)\n",
    "            result.append(e)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50674c",
   "metadata": {},
   "source": [
    "## 4. Process Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e27331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3685 total publications\n",
      "Processing 3685 publications:\n",
      "  - Manual labeled: 5\n",
      "  - Non-manual: 3680\n"
     ]
    }
   ],
   "source": [
    "all_pubs = sorted([p for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "print(f\"Found {len(all_pubs)} total publications\")\n",
    "\n",
    "# Separate manual and non-manual publications\n",
    "manual_pubs = [p for p in all_pubs if p.name in MANUAL_PUBS]\n",
    "non_manual_pubs = [p for p in all_pubs if p.name not in MANUAL_PUBS]\n",
    "\n",
    "# Process all publications\n",
    "publications = sorted(all_pubs, key=lambda p: p.name)\n",
    "\n",
    "print(f\"Processing {len(publications)} publications:\")\n",
    "print(f\"  - Manual labeled: {len(manual_pubs)}\")\n",
    "print(f\"  - Non-manual: {len(non_manual_pubs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a62e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2411-00222:\n",
      "  Refs (references.json): 16\n",
      "  Total .bib entries: 1\n",
      "  Actually cited bibs: 30 (filtered)\n",
      "  First bib: kumar2019failure - Failure modes in machine learning systems...\n"
     ]
    }
   ],
   "source": [
    "# Test on sample publication\n",
    "sample_pub = publications[0]\n",
    "refs = load_references_json(sample_pub / 'references.json')\n",
    "bibs = extract_bibs_from_publication(sample_pub)\n",
    "\n",
    "# Show stats\n",
    "tex_path = sample_pub / 'tex'\n",
    "total_bib_entries = 0\n",
    "for version_dir in tex_path.iterdir():\n",
    "    if version_dir.is_dir():\n",
    "        for bib_file in version_dir.rglob('*.bib'):\n",
    "            content = bib_file.read_text(encoding='utf-8', errors='ignore')\n",
    "            total_bib_entries += len(re.findall(r'@\\w+\\{', content))\n",
    "\n",
    "print(f\"{sample_pub.name}:\")\n",
    "print(f\"  Refs (references.json): {len(refs)}\")\n",
    "print(f\"  Total .bib entries: {total_bib_entries}\")\n",
    "print(f\"  Actually cited bibs: {len(bibs)} (filtered)\")\n",
    "if bibs:\n",
    "    print(f\"  First bib: {bibs[0].key} - {bibs[0].title[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b432b2e6",
   "metadata": {},
   "source": [
    "## 5. Extract All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d735f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3685/3685 [01:59<00:00, 30.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid: 2546, Skipped: 1139\n",
      "Refs filtered: 58132 → 56981 (2.0% removed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "def filter_refs_for_bibs(bibs: List[BibEntry], refs: Dict[str, RefEntry], max_title_diff: int = 40) -> Dict[str, RefEntry]:\n",
    "    \"\"\"Pre-filter refs based on title length compatibility\"\"\"\n",
    "    bib_lengths = [len((b.title or '').replace('\\n', ' ').strip()) for b in bibs]\n",
    "    return {k: v for k, v in refs.items() \n",
    "            if any(abs(len((v.title or '').replace('\\n', ' ').strip()) - bl) <= max_title_diff for bl in bib_lengths)}\n",
    "\n",
    "def process_publication(pub_path: Path) -> dict:\n",
    "    \"\"\"Process a single publication\"\"\"\n",
    "    result = {'status': 'skipped', 'reason': None, 'data': None, 'stats': {}}\n",
    "    \n",
    "    refs_path = pub_path / 'references.json'\n",
    "    if not refs_path.exists():\n",
    "        return {**result, 'reason': 'no_refs_file', 'pub_id': pub_path.name}\n",
    "    \n",
    "    refs = load_references_json(refs_path)\n",
    "    if not refs:\n",
    "        return {**result, 'reason': 'empty_refs', 'pub_id': pub_path.name}\n",
    "    \n",
    "    if not (pub_path / 'tex').exists():\n",
    "        return {**result, 'reason': 'no_tex_folder', 'pub_id': pub_path.name}\n",
    "    \n",
    "    bibs = extract_bibs_from_publication(pub_path)\n",
    "    if not bibs:\n",
    "        return {**result, 'reason': 'no_bibs', 'pub_id': pub_path.name}\n",
    "    \n",
    "    refs_before = len(refs)\n",
    "    filtered_refs = filter_refs_for_bibs(bibs, refs)\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'stats': {'refs_before': refs_before, 'refs_after': len(filtered_refs)},\n",
    "        'data': {\n",
    "            'pub_id': pub_path.name, 'num_refs': len(filtered_refs), 'num_bibs': len(bibs),\n",
    "            'refs': {k: v.to_dict() for k, v in filtered_refs.items()},\n",
    "            'bibs': [b.to_dict() for b in bibs]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Process with ThreadPoolExecutor\n",
    "num_workers = min(multiprocessing.cpu_count() * 2, 16)\n",
    "all_data, skipped = [], {'no_refs_file': [], 'empty_refs': [], 'no_bibs': [], 'no_tex_folder': []}\n",
    "total_refs_before, total_refs_after = 0, 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    futures = [executor.submit(process_publication, p) for p in publications]\n",
    "    for future in tqdm(as_completed(futures), total=len(publications), desc=\"Processing\"):\n",
    "        result = future.result()\n",
    "        if result['status'] == 'success':\n",
    "            all_data.append(result['data'])\n",
    "            total_refs_before += result['stats']['refs_before']\n",
    "            total_refs_after += result['stats']['refs_after']\n",
    "        elif result['reason']:\n",
    "            skipped[result['reason']].append(result.get('pub_id'))\n",
    "\n",
    "print(f\"\\nValid: {len(all_data)}, Skipped: {sum(len(v) for v in skipped.values())}\")\n",
    "print(f\"Refs filtered: {total_refs_before} → {total_refs_after} ({(total_refs_before - total_refs_after) / max(total_refs_before, 1) * 100:.1f}% removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7d3da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2546 publications to ..\\..\\output\\extracted_data.json\n"
     ]
    }
   ],
   "source": [
    "# Save extracted data\n",
    "with open(OUTPUT_DIR / 'extracted_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Saved {len(all_data)} publications to {OUTPUT_DIR / 'extracted_data.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac916232",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7faa834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications: 2546, Refs: 56981, Bibs: 122020\n",
      "Avg per pub: 22.4 refs, 47.9 bibs\n"
     ]
    }
   ],
   "source": [
    "total_refs = sum(d['num_refs'] for d in all_data)\n",
    "total_bibs = sum(d['num_bibs'] for d in all_data)\n",
    "print(f\"Publications: {len(all_data)}, Refs: {total_refs}, Bibs: {total_bibs}\")\n",
    "print(f\"Avg per pub: {total_refs/len(all_data):.1f} refs, {total_bibs/len(all_data):.1f} bibs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d3d84",
   "metadata": {},
   "source": [
    "## 7. Automatic Labeling\n",
    "Auto-label non-manual publications using regex and string similarity heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9169b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoLabeler:\n",
    "    \"\"\"Auto-labeling with regex and string similarity\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard(set1, set2):\n",
    "        return len(set1 & set2) / len(set1 | set2) if set1 and set2 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return set(re.sub(r'[^\\w\\s]', ' ', (text or '').lower()).split()) if text else set()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_last_name(author):\n",
    "        parts = (author or '').lower().split()\n",
    "        return parts[-1] if parts else \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_arxiv_match(content, refs):\n",
    "        \"\"\"Strategy 1: Exact arXiv ID in content\"\"\"\n",
    "        matches = []\n",
    "        for arxiv_id in refs.keys():\n",
    "            if arxiv_id.replace('-', '.') in content or arxiv_id in content:\n",
    "                matches.append((arxiv_id, 1.0, \"arxiv_exact\"))\n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_title_match(bib, refs, threshold=0.7):\n",
    "        \"\"\"Strategy 2: High title similarity\"\"\"\n",
    "        bib_tokens = AutoLabeler.tokenize(bib.get('title', ''))\n",
    "        if not bib_tokens:\n",
    "            return []\n",
    "        return [(arxiv_id, sim, \"title_jaccard\") \n",
    "                for arxiv_id, ref in refs.items()\n",
    "                if (sim := AutoLabeler.jaccard(bib_tokens, AutoLabeler.tokenize(ref.get('paper_title', '')))) >= threshold]\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_author_year_match(bib, refs):\n",
    "        \"\"\"Strategy 3: First author + year + partial title\"\"\"\n",
    "        bib_authors, bib_year = bib.get('authors', []), bib.get('year') or TextCleaner.extract_year(bib.get('raw_content', ''))\n",
    "        if not bib_authors or not bib_year:\n",
    "            return []\n",
    "        \n",
    "        bib_first = AutoLabeler.get_last_name(bib_authors[0])\n",
    "        bib_title_tokens = AutoLabeler.tokenize(bib.get('title', ''))\n",
    "        \n",
    "        matches = []\n",
    "        for arxiv_id, ref in refs.items():\n",
    "            ref_authors = ref.get('authors', [])\n",
    "            if not ref_authors or AutoLabeler.get_last_name(ref_authors[0]) != bib_first:\n",
    "                continue\n",
    "            try:\n",
    "                if abs(int(bib_year) - int(ref.get('submission_date', '')[:4])) > 1:\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                continue\n",
    "            overlap = AutoLabeler.jaccard(bib_title_tokens, AutoLabeler.tokenize(ref.get('paper_title', '')))\n",
    "            if overlap >= 0.3:\n",
    "                matches.append((arxiv_id, 0.6 + 0.4 * overlap, \"author_year_title\"))\n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def auto_label_publication(pub_data, refs_data):\n",
    "        \"\"\"Generate labels using all strategies\"\"\"\n",
    "        labels = {}\n",
    "        for bib in pub_data['bibs']:\n",
    "            raw = bib.get('raw_content', '')\n",
    "            matches = (AutoLabeler.find_arxiv_match(raw, refs_data) or \n",
    "                      AutoLabeler.find_title_match(bib, refs_data) or \n",
    "                      AutoLabeler.find_author_year_match(bib, refs_data))\n",
    "            if matches:\n",
    "                best = max(matches, key=lambda x: x[1])\n",
    "                labels[bib['key']] = {'arxiv_id': best[0], 'confidence': best[1], 'method': best[2]}\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "142f4d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-labeling: 100%|██████████| 2543/2543 [00:02<00:00, 897.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Auto-labeled: 1843/2543 publications (72.5%)\n",
      "Unlabeled: 700 publications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting unlabeled: 100%|██████████| 700/700 [00:01<00:00, 378.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 700 unlabeled publication directories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Run auto-labeling on non-manual publications\n",
    "non_manual_data = [d for d in all_data if d['pub_id'] not in MANUAL_PUBS]\n",
    "auto_labeled = []\n",
    "unlabeled_pubs = []\n",
    "\n",
    "for pub_data in tqdm(non_manual_data, desc=\"Auto-labeling\"):\n",
    "    labels = AutoLabeler.auto_label_publication(pub_data, pub_data['refs'])\n",
    "    if labels:\n",
    "        auto_labeled.append({'pub_id': pub_data['pub_id'], 'labels': labels, 'num_labels': len(labels)})\n",
    "    else:\n",
    "        unlabeled_pubs.append(pub_data['pub_id'])\n",
    "\n",
    "print(f\"\\nAuto-labeled: {len(auto_labeled)}/{len(non_manual_data)} publications ({len(auto_labeled)/len(non_manual_data)*100:.1f}%)\")\n",
    "print(f\"Unlabeled: {len(unlabeled_pubs)} publications\")\n",
    "\n",
    "# Delete unlabeled publication directories\n",
    "deleted_count = 0\n",
    "for pub_id in tqdm(unlabeled_pubs, desc=\"Deleting unlabeled\"):\n",
    "    pub_path = DATA_DIR / pub_id\n",
    "    if pub_path.exists():\n",
    "        shutil.rmtree(pub_path)\n",
    "        deleted_count += 1\n",
    "\n",
    "print(f\"Deleted {deleted_count} unlabeled publication directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca36b4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1843 pred.json files (test: 1, valid: 1, train: 1841)\n"
     ]
    }
   ],
   "source": [
    "# Assign partitions: test, valid, train\n",
    "AUTO_PARTITIONS = {}\n",
    "for i, result in enumerate(auto_labeled):\n",
    "    pub_id = result['pub_id']\n",
    "    AUTO_PARTITIONS[pub_id] = \"test\" if i == 0 else (\"valid\" if i == 1 else \"train\")\n",
    "\n",
    "# Save pred.json files\n",
    "for result in auto_labeled:\n",
    "    pub_id = result['pub_id']\n",
    "    partition = AUTO_PARTITIONS.get(pub_id, \"train\")\n",
    "    groundtruth = {k: v['arxiv_id'] for k, v in result['labels'].items()}\n",
    "    \n",
    "    pred_data = {\"partition\": partition, \"groundtruth\": groundtruth, \n",
    "                 \"prediction\": {k: [] for k in groundtruth}, \"label_source\": \"auto\"}\n",
    "    \n",
    "    with open(DATA_DIR / pub_id / \"pred.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(auto_labeled)} pred.json files (test: 1, valid: 1, train: {len(auto_labeled)-2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713b9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LABELING SUMMARY\n",
      "==================================================\n",
      "Manual: 5 publications (test: 2411-00222, valid: 2411-00223, train: 3)\n",
      "Auto: 1843 publications, 16140 labels\n",
      "Total extracted: 2546 publications\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"LABELING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Manual: {len(MANUAL_PUBS)} publications (test: 2411-00222, valid: 2411-00223, train: 3)\")\n",
    "print(f\"Auto: {len(auto_labeled)} publications, {sum(r['num_labels'] for r in auto_labeled)} labels\")\n",
    "print(f\"Total extracted: {len(all_data)} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5cb90",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** `02_feature_engineering.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
