{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51aef1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from matching import BibEntry, RefEntry, TextCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b83c7",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fb811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: ..\\..\\23120260, Output: ..\\..\\output\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../../23120260\")\n",
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Sampling configuration\n",
    "SAMPLE_SIZE = 1500\n",
    "RANDOM_SEED = 42\n",
    "MANUAL_PUBS = {\"2411-00222\", \"2411-00223\", \"2411-00225\", \"2411-00226\", \"2411-00227\"}\n",
    "\n",
    "print(f\"Data: {DATA_DIR}, Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01708ed",
   "metadata": {},
   "source": [
    "## 2. BibTeX Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a56cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibExtractor:\n",
    "    \"\"\"Extract BibTeX entries from LaTeX sources\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_bibitems(tex_content: str) -> List[BibEntry]:\n",
    "        entries = []\n",
    "        pattern = r'\\\\bibitem(?:\\[([^\\]]*)\\])?\\{([^}]+)\\}(.*?)(?=\\\\bibitem|\\\\end\\{thebibliography\\}|\\Z)'\n",
    "        \n",
    "        for match in re.finditer(pattern, tex_content, re.DOTALL):\n",
    "            key = match.group(2).strip()\n",
    "            content = match.group(3).strip()\n",
    "            entries.append(BibExtractor._parse_bibitem_content(key, content))\n",
    "        return entries\n",
    "    \n",
    "    @staticmethod\n",
    "    def _parse_bibitem_content(key: str, content: str) -> BibEntry:\n",
    "        content = re.sub(r'\\\\newblock\\s*', ' ', content)\n",
    "        content = re.sub(r'\\{\\\\em\\s+([^}]*)\\}', r'\\1', content)\n",
    "        content = re.sub(r'\\\\emph\\{([^}]*)\\}', r'\\1', content)\n",
    "        \n",
    "        arxiv_id = TextCleaner.extract_arxiv_id(content) or \"\"\n",
    "        year = TextCleaner.extract_year(content) or \"\"\n",
    "        \n",
    "        title = \"\"\n",
    "        title_match = re.search(r'\\{\\\\em\\s+([^}]+)\\}', content)\n",
    "        if title_match:\n",
    "            title = title_match.group(1)\n",
    "        elif '.' in content:\n",
    "            title = content.split('.')[1].strip()\n",
    "        \n",
    "        authors = []\n",
    "        author_part = content.split('.')[0] if '.' in content else content[:100]\n",
    "        for a in re.split(r'\\s+and\\s+|,\\s*(?=[A-Z])', author_part):\n",
    "            a = a.strip()\n",
    "            if a and len(a) > 2 and not a.startswith('\\\\'):\n",
    "                authors.append(a)\n",
    "        \n",
    "        return BibEntry(key=key, title=title, authors=authors[:10], year=year, \n",
    "                        arxiv_id=arxiv_id, raw_content=content[:500])\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_bib_file(bib_content: str) -> List[BibEntry]:\n",
    "        entries = []\n",
    "        pattern = r'@(\\w+)\\{([^,]+),\\s*(.*?)\\n\\}'\n",
    "        \n",
    "        for match in re.finditer(pattern, bib_content, re.DOTALL):\n",
    "            key = match.group(2).strip()\n",
    "            fields_str = match.group(3)\n",
    "            \n",
    "            fields = {}\n",
    "            for fm in re.finditer(r'(\\w+)\\s*=\\s*[{\"]([^}\"]*)[\"}]', fields_str, re.DOTALL):\n",
    "                fields[fm.group(1).lower()] = fm.group(2).strip()\n",
    "            \n",
    "            authors = re.split(r'\\s+and\\s+', fields.get('author', '')) if 'author' in fields else []\n",
    "            arxiv_id = fields.get('eprint', '') or fields.get('arxiv', '')\n",
    "            \n",
    "            entries.append(BibEntry(\n",
    "                key=key, title=fields.get('title', ''), authors=[a.strip() for a in authors],\n",
    "                year=fields.get('year', ''), venue=fields.get('journal', fields.get('booktitle', '')),\n",
    "                arxiv_id=arxiv_id.replace('.', '-'), raw_content=match.group(0)[:500]\n",
    "            ))\n",
    "        return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d807f3",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ec7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_references_json(path: Path) -> Dict[str, RefEntry]:\n",
    "    \"\"\"Load references.json into RefEntry objects\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return {arxiv_id: RefEntry(arxiv_id=arxiv_id, title=info.get('paper_title', ''),\n",
    "                               authors=info.get('authors', []), \n",
    "                               submission_date=info.get('submission_date', ''),\n",
    "                               venue=info.get('publication_venue', ''))\n",
    "            for arxiv_id, info in data.items()}\n",
    "\n",
    "\n",
    "def extract_bibs_from_publication(pub_path: Path) -> List[BibEntry]:\n",
    "    \"\"\"Extract all BibTeX entries from a publication\"\"\"\n",
    "    tex_path = pub_path / 'tex'\n",
    "    if not tex_path.exists():\n",
    "        return []\n",
    "    \n",
    "    all_entries = []\n",
    "    for version_dir in tex_path.iterdir():\n",
    "        if not version_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        for bib_file in version_dir.rglob('*.bib'):\n",
    "            try:\n",
    "                content = bib_file.read_text(encoding='utf-8', errors='ignore')\n",
    "                all_entries.extend(BibExtractor.extract_from_bib_file(content))\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        for tex_file in version_dir.rglob('*.tex'):\n",
    "            try:\n",
    "                content = tex_file.read_text(encoding='utf-8', errors='ignore')\n",
    "                if r'\\bibitem' in content:\n",
    "                    all_entries.extend(BibExtractor.extract_bibitems(content))\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Deduplicate by key\n",
    "    seen = set()\n",
    "    return [e for e in all_entries if not (e.key in seen or seen.add(e.key))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50674c",
   "metadata": {},
   "source": [
    "## 4. Process Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e27331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 total publications\n",
      "Selected: 5 manual + 1500 sampled = 1505 total\n"
     ]
    }
   ],
   "source": [
    "# Get and sample publications\n",
    "all_publications = sorted([p for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "print(f\"Found {len(all_publications)} total publications\")\n",
    "\n",
    "manual_pubs = [p for p in all_publications if p.name in MANUAL_PUBS]\n",
    "non_manual_pubs = [p for p in all_publications if p.name not in MANUAL_PUBS]\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "sampled = random.sample(non_manual_pubs, min(SAMPLE_SIZE, len(non_manual_pubs)))\n",
    "publications = sorted(manual_pubs + sampled, key=lambda p: p.name)\n",
    "\n",
    "print(f\"Selected: {len(manual_pubs)} manual + {len(sampled)} sampled = {len(publications)} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a62e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2411-00222: 16 refs, 31 bibs\n",
      "  First bib: ganjidoost2024protectingfeedforwardnetworksadversarial - Protecting Feed-Forward Networks from Adversarial Attacks Us...\n"
     ]
    }
   ],
   "source": [
    "# Test on sample publication\n",
    "sample_pub = publications[0]\n",
    "refs = load_references_json(sample_pub / 'references.json')\n",
    "bibs = extract_bibs_from_publication(sample_pub)\n",
    "print(f\"{sample_pub.name}: {len(refs)} refs, {len(bibs)} bibs\")\n",
    "if bibs:\n",
    "    print(f\"  First bib: {bibs[0].key} - {bibs[0].title[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b432b2e6",
   "metadata": {},
   "source": [
    "## 5. Extract All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d735f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 1505/1505 [00:51<00:00, 29.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid: 893, Skipped: 612\n",
      "Refs filtered: 20767 → 20408 (1.7% removed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "def filter_refs_for_bibs(bibs: List[BibEntry], refs: Dict[str, RefEntry], max_title_diff: int = 40) -> Dict[str, RefEntry]:\n",
    "    \"\"\"Pre-filter refs based on title length compatibility\"\"\"\n",
    "    bib_lengths = [len((b.title or '').replace('\\n', ' ').strip()) for b in bibs]\n",
    "    return {k: v for k, v in refs.items() \n",
    "            if any(abs(len((v.title or '').replace('\\n', ' ').strip()) - bl) <= max_title_diff for bl in bib_lengths)}\n",
    "\n",
    "def process_publication(pub_path: Path) -> dict:\n",
    "    \"\"\"Process a single publication\"\"\"\n",
    "    result = {'status': 'skipped', 'reason': None, 'data': None, 'stats': {}}\n",
    "    \n",
    "    refs_path = pub_path / 'references.json'\n",
    "    if not refs_path.exists():\n",
    "        return {**result, 'reason': 'no_refs_file', 'pub_id': pub_path.name}\n",
    "    \n",
    "    refs = load_references_json(refs_path)\n",
    "    if not refs:\n",
    "        return {**result, 'reason': 'empty_refs', 'pub_id': pub_path.name}\n",
    "    \n",
    "    if not (pub_path / 'tex').exists():\n",
    "        return {**result, 'reason': 'no_tex_folder', 'pub_id': pub_path.name}\n",
    "    \n",
    "    bibs = extract_bibs_from_publication(pub_path)\n",
    "    if not bibs:\n",
    "        return {**result, 'reason': 'no_bibs', 'pub_id': pub_path.name}\n",
    "    \n",
    "    refs_before = len(refs)\n",
    "    filtered_refs = filter_refs_for_bibs(bibs, refs)\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'stats': {'refs_before': refs_before, 'refs_after': len(filtered_refs)},\n",
    "        'data': {\n",
    "            'pub_id': pub_path.name, 'num_refs': len(filtered_refs), 'num_bibs': len(bibs),\n",
    "            'refs': {k: v.to_dict() for k, v in filtered_refs.items()},\n",
    "            'bibs': [b.to_dict() for b in bibs]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Process with ThreadPoolExecutor\n",
    "num_workers = min(multiprocessing.cpu_count() * 2, 16)\n",
    "all_data, skipped = [], {'no_refs_file': [], 'empty_refs': [], 'no_bibs': [], 'no_tex_folder': []}\n",
    "total_refs_before, total_refs_after = 0, 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    futures = [executor.submit(process_publication, p) for p in publications]\n",
    "    for future in tqdm(as_completed(futures), total=len(publications), desc=\"Processing\"):\n",
    "        result = future.result()\n",
    "        if result['status'] == 'success':\n",
    "            all_data.append(result['data'])\n",
    "            total_refs_before += result['stats']['refs_before']\n",
    "            total_refs_after += result['stats']['refs_after']\n",
    "        elif result['reason']:\n",
    "            skipped[result['reason']].append(result.get('pub_id'))\n",
    "\n",
    "print(f\"\\nValid: {len(all_data)}, Skipped: {sum(len(v) for v in skipped.values())}\")\n",
    "print(f\"Refs filtered: {total_refs_before} → {total_refs_after} ({(total_refs_before - total_refs_after) / max(total_refs_before, 1) * 100:.1f}% removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7d3da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 893 publications to ..\\..\\output\\extracted_data.json\n"
     ]
    }
   ],
   "source": [
    "# Save extracted data\n",
    "with open(OUTPUT_DIR / 'extracted_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Saved {len(all_data)} publications to {OUTPUT_DIR / 'extracted_data.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac916232",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7faa834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications: 893, Refs: 20408, Bibs: 422393\n",
      "Avg per pub: 22.9 refs, 473.0 bibs\n"
     ]
    }
   ],
   "source": [
    "total_refs = sum(d['num_refs'] for d in all_data)\n",
    "total_bibs = sum(d['num_bibs'] for d in all_data)\n",
    "print(f\"Publications: {len(all_data)}, Refs: {total_refs}, Bibs: {total_bibs}\")\n",
    "print(f\"Avg per pub: {total_refs/len(all_data):.1f} refs, {total_bibs/len(all_data):.1f} bibs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d3d84",
   "metadata": {},
   "source": [
    "## 7. Automatic Labeling\n",
    "Auto-label non-manual publications using regex and string similarity heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9169b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoLabeler:\n",
    "    \"\"\"Auto-labeling with regex and string similarity\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard(set1, set2):\n",
    "        return len(set1 & set2) / len(set1 | set2) if set1 and set2 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return set(re.sub(r'[^\\w\\s]', ' ', (text or '').lower()).split()) if text else set()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_last_name(author):\n",
    "        parts = (author or '').lower().split()\n",
    "        return parts[-1] if parts else \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_arxiv_match(content, refs):\n",
    "        \"\"\"Strategy 1: Exact arXiv ID in content\"\"\"\n",
    "        matches = []\n",
    "        for arxiv_id in refs.keys():\n",
    "            if arxiv_id.replace('-', '.') in content or arxiv_id in content:\n",
    "                matches.append((arxiv_id, 1.0, \"arxiv_exact\"))\n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_title_match(bib, refs, threshold=0.7):\n",
    "        \"\"\"Strategy 2: High title similarity\"\"\"\n",
    "        bib_tokens = AutoLabeler.tokenize(bib.get('title', ''))\n",
    "        if not bib_tokens:\n",
    "            return []\n",
    "        return [(arxiv_id, sim, \"title_jaccard\") \n",
    "                for arxiv_id, ref in refs.items()\n",
    "                if (sim := AutoLabeler.jaccard(bib_tokens, AutoLabeler.tokenize(ref.get('paper_title', '')))) >= threshold]\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_author_year_match(bib, refs):\n",
    "        \"\"\"Strategy 3: First author + year + partial title\"\"\"\n",
    "        bib_authors, bib_year = bib.get('authors', []), bib.get('year') or TextCleaner.extract_year(bib.get('raw_content', ''))\n",
    "        if not bib_authors or not bib_year:\n",
    "            return []\n",
    "        \n",
    "        bib_first = AutoLabeler.get_last_name(bib_authors[0])\n",
    "        bib_title_tokens = AutoLabeler.tokenize(bib.get('title', ''))\n",
    "        \n",
    "        matches = []\n",
    "        for arxiv_id, ref in refs.items():\n",
    "            ref_authors = ref.get('authors', [])\n",
    "            if not ref_authors or AutoLabeler.get_last_name(ref_authors[0]) != bib_first:\n",
    "                continue\n",
    "            try:\n",
    "                if abs(int(bib_year) - int(ref.get('submission_date', '')[:4])) > 1:\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                continue\n",
    "            overlap = AutoLabeler.jaccard(bib_title_tokens, AutoLabeler.tokenize(ref.get('paper_title', '')))\n",
    "            if overlap >= 0.3:\n",
    "                matches.append((arxiv_id, 0.6 + 0.4 * overlap, \"author_year_title\"))\n",
    "        return matches\n",
    "    \n",
    "    @staticmethod\n",
    "    def auto_label_publication(pub_data, refs_data):\n",
    "        \"\"\"Generate labels using all strategies\"\"\"\n",
    "        labels = {}\n",
    "        for bib in pub_data['bibs']:\n",
    "            raw = bib.get('raw_content', '')\n",
    "            matches = (AutoLabeler.find_arxiv_match(raw, refs_data) or \n",
    "                      AutoLabeler.find_title_match(bib, refs_data) or \n",
    "                      AutoLabeler.find_author_year_match(bib, refs_data))\n",
    "            if matches:\n",
    "                best = max(matches, key=lambda x: x[1])\n",
    "                labels[bib['key']] = {'arxiv_id': best[0], 'confidence': best[1], 'method': best[2]}\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "142f4d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-labeling: 100%|██████████| 888/888 [00:08<00:00, 107.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Auto-labeled: 664/888 publications (74.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run auto-labeling on non-manual publications\n",
    "non_manual_data = [d for d in all_data if d['pub_id'] not in MANUAL_PUBS]\n",
    "auto_labeled = []\n",
    "\n",
    "for pub_data in tqdm(non_manual_data, desc=\"Auto-labeling\"):\n",
    "    labels = AutoLabeler.auto_label_publication(pub_data, pub_data['refs'])\n",
    "    if labels:\n",
    "        auto_labeled.append({'pub_id': pub_data['pub_id'], 'labels': labels, 'num_labels': len(labels)})\n",
    "\n",
    "print(f\"\\nAuto-labeled: {len(auto_labeled)}/{len(non_manual_data)} publications ({len(auto_labeled)/len(non_manual_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca36b4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 664 pred.json files (test: 1, valid: 1, train: 662)\n"
     ]
    }
   ],
   "source": [
    "# Assign partitions: test, valid, train\n",
    "AUTO_PARTITIONS = {}\n",
    "for i, result in enumerate(auto_labeled):\n",
    "    pub_id = result['pub_id']\n",
    "    AUTO_PARTITIONS[pub_id] = \"test\" if i == 0 else (\"valid\" if i == 1 else \"train\")\n",
    "\n",
    "# Save pred.json files\n",
    "for result in auto_labeled:\n",
    "    pub_id = result['pub_id']\n",
    "    partition = AUTO_PARTITIONS.get(pub_id, \"train\")\n",
    "    groundtruth = {k: v['arxiv_id'] for k, v in result['labels'].items()}\n",
    "    \n",
    "    pred_data = {\"partition\": partition, \"groundtruth\": groundtruth, \n",
    "                 \"prediction\": {k: [] for k in groundtruth}, \"label_source\": \"auto\"}\n",
    "    \n",
    "    with open(DATA_DIR / pub_id / \"pred.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(auto_labeled)} pred.json files (test: 1, valid: 1, train: {len(auto_labeled)-2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713b9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LABELING SUMMARY\n",
      "==================================================\n",
      "Manual: 5 publications (test: 2411-00222, valid: 2411-00223, train: 3)\n",
      "Auto: 664 publications, 5891 labels\n",
      "Total extracted: 893 publications\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"LABELING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Manual: {len(MANUAL_PUBS)} publications (test: 2411-00222, valid: 2411-00223, train: 3)\")\n",
    "print(f\"Auto: {len(auto_labeled)} publications, {sum(r['num_labels'] for r in auto_labeled)} labels\")\n",
    "print(f\"Total extracted: {len(all_data)} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5cb90",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** `02_feature_engineering.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
