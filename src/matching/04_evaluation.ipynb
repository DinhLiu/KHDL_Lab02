{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d538f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from matching.common import (\n",
    "    ReferenceMatchingModel, FeatureExtractor, TextCleaner,\n",
    "    load_ground_truth, calculate_mrr, calculate_precision_at_k,\n",
    "    DATA_DIR, OUTPUT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d56c66",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004558c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# DATA_DIR and OUTPUT_DIR imported from common.py\n",
    "\n",
    "model = ReferenceMatchingModel()\n",
    "model.load(OUTPUT_DIR / 'reference_matching_model.json')\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff8b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2546 publications\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIR / 'extracted_data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "print(f\"Loaded {len(all_data)} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd14e6e",
   "metadata": {},
   "source": [
    "## 2. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf535ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: MRR@5, P@1, P@3, P@5\n"
     ]
    }
   ],
   "source": [
    "# Evaluation functions imported from common.py\n",
    "# calculate_mrr(predictions, ground_truth, top_k)\n",
    "# calculate_precision_at_k(predictions, ground_truth, k)\n",
    "print(\"Metrics: MRR@5, P@1, P@3, P@5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c046f",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction (from notebook 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4c21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextCleaner and FeatureExtractor imported from common.py\n",
    "# Helper function to extract features as numpy array\n",
    "def extract_features(bib: Dict, ref: Dict) -> np.ndarray:\n",
    "    return FeatureExtractor.extract_features_vector(bib, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c1b3d",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5fa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(bib: Dict, refs: Dict[str, Dict], model, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Use model's built-in rank_candidates method\"\"\"\n",
    "    return model.rank_candidates(bib, refs, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed78852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 1854 pubs, Test: 2, Valid: 2\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth from pred.json files (using common.py)\n",
    "ground_truth_lookup, partition_lookup = load_ground_truth(DATA_DIR)\n",
    "\n",
    "print(f\"Ground truth: {len(ground_truth_lookup)} pubs, \"\n",
    "      f\"Test: {sum(1 for p in partition_lookup.values() if p == 'test')}, \"\n",
    "      f\"Valid: {sum(1 for p in partition_lookup.values() if p == 'valid')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b002f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 2 pubs, 16 entries\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for TEST SET\n",
    "test_predictions, test_ground_truth, test_pub_ids = [], [], []\n",
    "\n",
    "for pub_data in all_data:\n",
    "    pub_id = pub_data['pub_id']\n",
    "    if partition_lookup.get(pub_id) != 'test' or pub_id not in ground_truth_lookup:\n",
    "        continue\n",
    "    \n",
    "    for bib in pub_data['bibs']:\n",
    "        if bib['key'] not in ground_truth_lookup[pub_id]:\n",
    "            continue\n",
    "        ranked = rank_candidates(bib, pub_data['refs'], model, top_k=5)\n",
    "        test_predictions.append([arxiv_id for arxiv_id, _ in ranked])\n",
    "        test_ground_truth.append(ground_truth_lookup[pub_id][bib['key']])\n",
    "        test_pub_ids.append(pub_id)\n",
    "\n",
    "print(f\"Test: {len(set(test_pub_ids))} pubs, {len(test_predictions)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07439e66",
   "metadata": {},
   "source": [
    "## 5. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ce10de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TEST SET METRICS\n",
      "==================================================\n",
      "MRR@5: 0.9375, P@1: 0.8750, P@3: 1.0000, P@5: 1.0000\n",
      "  2411-00222: MRR@5=0.9333, P@1=0.8667\n",
      "  2411-00232: MRR@5=1.0000, P@1=1.0000\n"
     ]
    }
   ],
   "source": [
    "# MRR@5 and Precision metrics\n",
    "mrr_5 = calculate_mrr(test_predictions, test_ground_truth, top_k=5)\n",
    "p_at_1 = calculate_precision_at_k(test_predictions, test_ground_truth, k=1)\n",
    "p_at_3 = calculate_precision_at_k(test_predictions, test_ground_truth, k=3)\n",
    "p_at_5 = calculate_precision_at_k(test_predictions, test_ground_truth, k=5)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TEST SET METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MRR@5: {mrr_5:.4f}, P@1: {p_at_1:.4f}, P@3: {p_at_3:.4f}, P@5: {p_at_5:.4f}\")\n",
    "\n",
    "for pub in sorted(set(test_pub_ids)):\n",
    "    pub_preds = [p for p, pid in zip(test_predictions, test_pub_ids) if pid == pub]\n",
    "    pub_gt = [g for g, pid in zip(test_ground_truth, test_pub_ids) if pid == pub]\n",
    "    print(f\"  {pub}: MRR@5={calculate_mrr(pub_preds, pub_gt):.4f}, P@1={calculate_precision_at_k(pub_preds, pub_gt):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01c2ea",
   "metadata": {},
   "source": [
    "## 6. Generate pred.json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc23584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_json(pub_path: Path, pub_data: Dict, gt_dict: Dict, model, partition: str = \"test\"):\n",
    "    \"\"\"Generate pred.json with top-5 predictions\"\"\"\n",
    "    pred_structure = {\"partition\": partition, \"groundtruth\": {}, \"prediction\": {}}\n",
    "    \n",
    "    for bib_key, arxiv_id in gt_dict.items():\n",
    "        pred_structure[\"groundtruth\"][bib_key] = arxiv_id\n",
    "        bib = next((b for b in pub_data['bibs'] if b['key'] == bib_key), None)\n",
    "        if bib:\n",
    "            ranked = rank_candidates(bib, pub_data['refs'], model, top_k=5)\n",
    "            pred_structure[\"prediction\"][bib_key] = [aid for aid, _ in ranked]\n",
    "        else:\n",
    "            pred_structure[\"prediction\"][bib_key] = []\n",
    "    \n",
    "    with open(pub_path / 'pred.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_structure, f, indent=2)\n",
    "    return pub_path / 'pred.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b97bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1846 pred.json files: Test=2, Valid=2, Train=1842\n"
     ]
    }
   ],
   "source": [
    "# Generate pred.json for all publications with ground truth\n",
    "generated, partition_counts = 0, {'test': 0, 'valid': 0, 'train': 0}\n",
    "\n",
    "for pub_data in all_data:\n",
    "    pub_id = pub_data['pub_id']\n",
    "    if pub_id not in ground_truth_lookup:\n",
    "        continue\n",
    "    partition = partition_lookup.get(pub_id, 'train')\n",
    "    generate_pred_json(DATA_DIR / pub_id, pub_data, ground_truth_lookup[pub_id], model, partition)\n",
    "    generated += 1\n",
    "    partition_counts[partition] += 1\n",
    "\n",
    "print(f\"Generated {generated} pred.json files: Test={partition_counts['test']}, Valid={partition_counts['valid']}, Train={partition_counts['train']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d402b",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42345e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "Publications: 1854 with labels\n",
      "Test entries: 16\n",
      "MRR@5: 0.9375, P@1: 0.8750\n",
      "Output: 1846 pred.json files\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Publications: {len(ground_truth_lookup)} with labels\")\n",
    "print(f\"Test entries: {len(test_predictions)}\")\n",
    "print(f\"MRR@5: {mrr_5:.4f}, P@1: {p_at_1:.4f}\")\n",
    "print(f\"Output: {generated} pred.json files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cf6dc",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Complete!\n",
    "\n",
    "| Requirement | Implementation |\n",
    "|-------------|----------------|\n",
    "| 2.2.1 Data Cleaning | Lowercasing, tokenization, LaTeX cleanup |\n",
    "| 2.2.2 Labels | 5 manual + auto-labeled (regex/similarity) |\n",
    "| 2.2.3 Features | 12 features: title, author, year, arXiv |\n",
    "| 2.2.4 Data Split | Publication-level: test/valid/train |\n",
    "| 2.2.5 Evaluation | MRR@5 on test set |\n",
    "\n",
    "**pred.json format:** `{partition, groundtruth, prediction}`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
