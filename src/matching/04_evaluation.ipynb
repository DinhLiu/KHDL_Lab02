{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d538f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d56c66",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004558c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../../23120260\")\n",
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "\n",
    "# Load model\n",
    "class ReferenceMatchingModel:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = 0.0\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        linear = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(linear)\n",
    "    \n",
    "    def load(self, path: Path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.weights = np.array(data['weights']) if data['weights'] else None\n",
    "        self.bias = data['bias']\n",
    "\n",
    "model = ReferenceMatchingModel()\n",
    "model.load(OUTPUT_DIR / 'reference_matching_model.json')\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extracted data\n",
    "with open(OUTPUT_DIR / 'extracted_data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# Load manual labels\n",
    "manual_labels = []\n",
    "if (OUTPUT_DIR / 'manual_labels.json').exists():\n",
    "    with open(OUTPUT_DIR / 'manual_labels.json', 'r', encoding='utf-8') as f:\n",
    "        manual_labels = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(all_data)} publications\")\n",
    "print(f\"Loaded {len(manual_labels)} manual labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd14e6e",
   "metadata": {},
   "source": [
    "## 2. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf535ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(predictions: List[List[str]], ground_truth: List[str], top_k: int = 5) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR).\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of ranked prediction lists (arxiv IDs)\n",
    "        ground_truth: List of correct arxiv IDs\n",
    "        top_k: Consider only top-k predictions\n",
    "    \n",
    "    Returns:\n",
    "        MRR score\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for pred_list, true_id in zip(predictions, ground_truth):\n",
    "        pred_list = pred_list[:top_k]\n",
    "        \n",
    "        rank = 0\n",
    "        for i, pred_id in enumerate(pred_list):\n",
    "            if pred_id == true_id:\n",
    "                rank = i + 1\n",
    "                break\n",
    "        \n",
    "        reciprocal_ranks.append(1.0 / rank if rank > 0 else 0.0)\n",
    "    \n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "def calculate_precision_at_k(predictions: List[List[str]], ground_truth: List[str], k: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Precision@K.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of ranked prediction lists\n",
    "        ground_truth: List of correct IDs\n",
    "        k: Consider top-k predictions\n",
    "    \n",
    "    Returns:\n",
    "        Precision@K score\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for pred_list, true_id in zip(predictions, ground_truth):\n",
    "        if true_id in pred_list[:k]:\n",
    "            correct += 1\n",
    "    return correct / len(predictions) if predictions else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c046f",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction (from notebook 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Set\n",
    "\n",
    "class TextCleaner:\n",
    "    STOP_WORDS = {'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_title(title: str) -> str:\n",
    "        title = TextCleaner.clean_text(title)\n",
    "        title = re.sub(r'^(on|the|a|an)\\s+', '', title)\n",
    "        return title\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        text = TextCleaner.clean_text(text)\n",
    "        return [t for t in text.split() if t not in TextCleaner.STOP_WORDS]\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_author_last_names(authors: List[str]) -> List[str]:\n",
    "        result = []\n",
    "        for author in authors:\n",
    "            if author:\n",
    "                cleaned = TextCleaner.clean_text(author)\n",
    "                parts = cleaned.split()\n",
    "                if parts:\n",
    "                    result.append(parts[-1])\n",
    "        return result\n",
    "\n",
    "\n",
    "def extract_features(bib: Dict, ref: Dict) -> np.ndarray:\n",
    "    \"\"\"Extract feature vector for a bib-ref pair\"\"\"\n",
    "    # Title features\n",
    "    bib_title = TextCleaner.clean_title(bib.get('title', ''))\n",
    "    ref_title = TextCleaner.clean_title(ref.get('title', ''))\n",
    "    bib_tokens = set(TextCleaner.tokenize(bib_title))\n",
    "    ref_tokens = set(TextCleaner.tokenize(ref_title))\n",
    "    \n",
    "    # Jaccard\n",
    "    if bib_tokens and ref_tokens:\n",
    "        jaccard = len(bib_tokens & ref_tokens) / len(bib_tokens | ref_tokens)\n",
    "    else:\n",
    "        jaccard = 0.0\n",
    "    \n",
    "    # Overlap\n",
    "    if bib_tokens and ref_tokens:\n",
    "        overlap = len(bib_tokens & ref_tokens) / min(len(bib_tokens), len(ref_tokens))\n",
    "    else:\n",
    "        overlap = 0.0\n",
    "    \n",
    "    # Edit distance (simplified)\n",
    "    edit_sim = 1.0 - (abs(len(bib_title) - len(ref_title)) / max(len(bib_title), len(ref_title), 1))\n",
    "    \n",
    "    # Author features\n",
    "    bib_authors = set(TextCleaner.extract_author_last_names(bib.get('authors', [])))\n",
    "    ref_authors = set(TextCleaner.extract_author_last_names(ref.get('authors', [])))\n",
    "    \n",
    "    if bib_authors and ref_authors:\n",
    "        author_overlap = len(bib_authors & ref_authors) / min(len(bib_authors), len(ref_authors))\n",
    "    else:\n",
    "        author_overlap = 0.0\n",
    "    \n",
    "    bib_authors_list = list(bib_authors)\n",
    "    ref_authors_list = list(ref_authors)\n",
    "    first_author_match = 1.0 if (bib_authors_list and ref_authors_list and \n",
    "                                  bib_authors_list[0] == ref_authors_list[0]) else 0.0\n",
    "    \n",
    "    # Year features\n",
    "    bib_year = bib.get('year', '')\n",
    "    ref_year = ref.get('year', '')\n",
    "    year_match = 1.0 if bib_year == ref_year else 0.0\n",
    "    try:\n",
    "        year_diff = abs(int(bib_year) - int(ref_year)) if bib_year and ref_year else 10\n",
    "    except:\n",
    "        year_diff = 10\n",
    "    \n",
    "    # ArXiv features\n",
    "    bib_arxiv = (bib.get('arxiv_id') or '').replace('.', '-')\n",
    "    ref_arxiv = (ref.get('arxiv_id') or '').replace('.', '-')\n",
    "    arxiv_match = 1.0 if (bib_arxiv and ref_arxiv and bib_arxiv == ref_arxiv) else 0.0\n",
    "    \n",
    "    raw_content = bib.get('raw_content', '')\n",
    "    arxiv_in_content = 1.0 if ref_arxiv.replace('-', '.') in raw_content else 0.0\n",
    "    \n",
    "    num_matching_authors = len(bib_authors & ref_authors)\n",
    "    \n",
    "    len_ratio = len(bib_title) / len(ref_title) if ref_title else 0\n",
    "    title_len_ratio = min(len_ratio, 1/len_ratio) if len_ratio > 0 else 0\n",
    "    \n",
    "    combined_score = 0.4 * jaccard + 0.3 * author_overlap + 0.2 * year_match + 0.1 * first_author_match\n",
    "    \n",
    "    return np.array([\n",
    "        jaccard, overlap, edit_sim,\n",
    "        author_overlap, first_author_match,\n",
    "        year_match, year_diff,\n",
    "        arxiv_match, arxiv_in_content,\n",
    "        num_matching_authors, title_len_ratio, combined_score\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c1b3d",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(bib: Dict, refs: Dict[str, Dict], model: ReferenceMatchingModel, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Rank reference candidates for a BibTeX entry\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for arxiv_id, ref in refs.items():\n",
    "        features = extract_features(bib, ref)\n",
    "        score = model.predict_proba(features.reshape(1, -1))[0]\n",
    "        scores.append((arxiv_id, score))\n",
    "    \n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed78852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load ground truth from pred.json files (supports both manual and auto labels)\n",
    "# ============================================================================\n",
    "\n",
    "ground_truth_lookup = defaultdict(dict)\n",
    "partition_lookup = {}\n",
    "\n",
    "for pub_data in all_data:\n",
    "    pub_id = pub_data['pub_id']\n",
    "    pub_path = DATA_DIR / pub_id\n",
    "    pred_file = pub_path / \"pred.json\"\n",
    "    \n",
    "    if pred_file.exists():\n",
    "        with open(pred_file, 'r') as f:\n",
    "            pred_data = json.load(f)\n",
    "        \n",
    "        partition = pred_data.get('partition', 'train')\n",
    "        partition_lookup[pub_id] = partition\n",
    "        \n",
    "        for bib_key, arxiv_id in pred_data.get('groundtruth', {}).items():\n",
    "            ground_truth_lookup[pub_id][bib_key] = arxiv_id\n",
    "\n",
    "print(f\"Ground truth for {len(ground_truth_lookup)} publications\")\n",
    "print(f\"  Test: {sum(1 for p in partition_lookup.values() if p == 'test')}\")\n",
    "print(f\"  Valid: {sum(1 for p in partition_lookup.values() if p == 'valid')}\")\n",
    "print(f\"  Train: {sum(1 for p in partition_lookup.values() if p == 'train')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Generate predictions for TEST SET ONLY (Requirement 2.2.5)\n",
    "# Test set = 2 publications (1 manual + 1 auto-labeled)\n",
    "# ============================================================================\n",
    "\n",
    "test_predictions = []\n",
    "test_ground_truth = []\n",
    "test_pub_ids = []\n",
    "\n",
    "for pub_data in all_data:\n",
    "    pub_id = pub_data['pub_id']\n",
    "    \n",
    "    # Only evaluate on TEST partition\n",
    "    if partition_lookup.get(pub_id) != 'test':\n",
    "        continue\n",
    "    \n",
    "    if pub_id not in ground_truth_lookup:\n",
    "        continue\n",
    "    \n",
    "    gt_dict = ground_truth_lookup[pub_id]\n",
    "    \n",
    "    for bib in pub_data['bibs']:\n",
    "        bib_key = bib['key']\n",
    "        \n",
    "        if bib_key not in gt_dict:\n",
    "            continue\n",
    "        \n",
    "        # Get ground truth\n",
    "        true_arxiv = gt_dict[bib_key]\n",
    "        \n",
    "        # Rank candidates (top 5 as per requirement)\n",
    "        ranked = rank_candidates(bib, pub_data['refs'], model, top_k=5)\n",
    "        pred_list = [arxiv_id for arxiv_id, score in ranked]\n",
    "        \n",
    "        test_predictions.append(pred_list)\n",
    "        test_ground_truth.append(true_arxiv)\n",
    "        test_pub_ids.append(pub_id)\n",
    "\n",
    "print(f\"Test set evaluation:\")\n",
    "print(f\"  Publications: {len(set(test_pub_ids))}\")\n",
    "print(f\"  Total entries: {len(test_predictions)}\")\n",
    "for pub in sorted(set(test_pub_ids)):\n",
    "    count = sum(1 for p in test_pub_ids if p == pub)\n",
    "    print(f\"    - {pub}: {count} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07439e66",
   "metadata": {},
   "source": [
    "## 5. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Compute MRR@5 (Requirement 2.2.5)\n",
    "# ============================================================================\n",
    "# MRR = (1/|Q|) * sum(1/rank_i) where rank_i is position of correct match\n",
    "\n",
    "mrr_5 = calculate_mrr(test_predictions, test_ground_truth, top_k=5)\n",
    "p_at_1 = calculate_precision_at_k(test_predictions, test_ground_truth, k=1)\n",
    "p_at_3 = calculate_precision_at_k(test_predictions, test_ground_truth, k=3)\n",
    "p_at_5 = calculate_precision_at_k(test_predictions, test_ground_truth, k=5)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION METRICS (Test Set Only)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMRR@5: {mrr_5:.4f}\")\n",
    "print(f\"Precision@1: {p_at_1:.4f}\")\n",
    "print(f\"Precision@3: {p_at_3:.4f}\")\n",
    "print(f\"Precision@5: {p_at_5:.4f}\")\n",
    "\n",
    "# Detailed breakdown by publication\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"Breakdown by Test Publication:\")\n",
    "for pub in sorted(set(test_pub_ids)):\n",
    "    pub_preds = [p for p, pid in zip(test_predictions, test_pub_ids) if pid == pub]\n",
    "    pub_gt = [g for g, pid in zip(test_ground_truth, test_pub_ids) if pid == pub]\n",
    "    pub_mrr = calculate_mrr(pub_preds, pub_gt, top_k=5)\n",
    "    pub_p1 = calculate_precision_at_k(pub_preds, pub_gt, k=1)\n",
    "    print(f\"  {pub}: MRR@5={pub_mrr:.4f}, P@1={pub_p1:.4f} ({len(pub_preds)} entries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01c2ea",
   "metadata": {},
   "source": [
    "## 6. Generate pred.json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc23584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_json(pub_path: Path, pub_data: Dict, ground_truth_dict: Dict, \n",
    "                       model: ReferenceMatchingModel, partition: str = \"test\"):\n",
    "    \"\"\"\n",
    "    Generate pred.json for a publication (Requirement 3.1.3).\n",
    "    \n",
    "    Format:\n",
    "    {\n",
    "        \"partition\": \"test\" | \"valid\" | \"train\",\n",
    "        \"groundtruth\": {\"bibtex_entry_name_1\": \"arxiv_id_from_references_json\", ...},\n",
    "        \"prediction\": {\"bibtex_entry_name_1\": [\"candidate_id_1\", \"candidate_id_2\", ...], ...}\n",
    "    }\n",
    "    \"\"\"\n",
    "    pred_structure = {\n",
    "        \"partition\": partition,\n",
    "        \"groundtruth\": {},\n",
    "        \"prediction\": {}\n",
    "    }\n",
    "    \n",
    "    # Add ground truth and predictions\n",
    "    for bib_key, arxiv_id in ground_truth_dict.items():\n",
    "        pred_structure[\"groundtruth\"][bib_key] = arxiv_id\n",
    "        \n",
    "        # Find bib entry and generate top-5 predictions\n",
    "        bib_entry = None\n",
    "        for bib in pub_data['bibs']:\n",
    "            if bib['key'] == bib_key:\n",
    "                bib_entry = bib\n",
    "                break\n",
    "        \n",
    "        if bib_entry:\n",
    "            # Top 5 ranked candidates as per requirement\n",
    "            ranked = rank_candidates(bib_entry, pub_data['refs'], model, top_k=5)\n",
    "            pred_structure[\"prediction\"][bib_key] = [arxiv_id for arxiv_id, _ in ranked]\n",
    "        else:\n",
    "            pred_structure[\"prediction\"][bib_key] = []\n",
    "    \n",
    "    # Save pred.json\n",
    "    pred_path = pub_path / 'pred.json'\n",
    "    with open(pred_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_structure, f, indent=2)\n",
    "    \n",
    "    return pred_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pred.json for ALL publications with ground truth\n",
    "# Include partition information from lookup\n",
    "\n",
    "generated_count = 0\n",
    "partition_counts = {'test': 0, 'valid': 0, 'train': 0}\n",
    "\n",
    "for pub_data in all_data:\n",
    "    pub_id = pub_data['pub_id']\n",
    "    \n",
    "    if pub_id not in ground_truth_lookup:\n",
    "        continue\n",
    "    \n",
    "    pub_path = DATA_DIR / pub_id\n",
    "    gt_dict = ground_truth_lookup[pub_id]\n",
    "    partition = partition_lookup.get(pub_id, 'train')\n",
    "    \n",
    "    generate_pred_json(pub_path, pub_data, gt_dict, model, partition=partition)\n",
    "    generated_count += 1\n",
    "    partition_counts[partition] += 1\n",
    "\n",
    "print(f\"Generated {generated_count} pred.json files\")\n",
    "print(f\"  Test: {partition_counts['test']}\")\n",
    "print(f\"  Valid: {partition_counts['valid']}\")\n",
    "print(f\"  Train: {partition_counts['train']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d402b",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42345e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY (Requirement 2.2.5)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total publications with labels: {len(ground_truth_lookup)}\")\n",
    "print(f\"  Test publications: {sum(1 for p in partition_lookup.values() if p == 'test')}\")\n",
    "print(f\"  Valid publications: {sum(1 for p in partition_lookup.values() if p == 'valid')}\")\n",
    "print(f\"  Train publications: {sum(1 for p in partition_lookup.values() if p == 'train')}\")\n",
    "\n",
    "print(f\"\\nTest Set Metrics (MRR@5 as per requirement):\")\n",
    "print(f\"  Test entries evaluated: {len(test_predictions)}\")\n",
    "print(f\"  MRR@5: {mrr_5:.4f}\")\n",
    "print(f\"  Precision@1: {p_at_1:.4f}\")\n",
    "print(f\"  Precision@3: {p_at_3:.4f}\")\n",
    "print(f\"  Precision@5: {p_at_5:.4f}\")\n",
    "\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  {generated_count} pred.json files generated\")\n",
    "print(f\"    Format: partition, groundtruth, prediction (top 5 candidates)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MRR Formula: (1/|Q|) * Î£(1/rank_i)\")\n",
    "print(\"  |Q| = total references to match\")\n",
    "print(\"  rank_i = position of correct match in top-5 list (0 if not found)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cf6dc",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Complete!\n",
    "\n",
    "The reference matching pipeline implements all requirements from Section 2.2:\n",
    "\n",
    "| Requirement | Implementation |\n",
    "|-------------|----------------|\n",
    "| **2.2.1 Data Cleaning** | Lowercasing, stop-word removal, tokenization, LaTeX cleanup |\n",
    "| **2.2.2 Manual Labels** | 5 publications, 52+ label pairs (exceeds 20 minimum) |\n",
    "| **2.2.2 Auto Labels** | 10%+ of non-manual data with regex/similarity heuristics |\n",
    "| **2.2.3 Features** | 12 features: title similarity, author overlap, year match, arXiv ID |\n",
    "| **2.2.4 Data Split** | Publication-level: Test (1 manual + 1 auto), Valid (1 manual + 1 auto), Train (rest) |\n",
    "| **2.2.5 Evaluation** | MRR@5 on Test set (2 publications) |\n",
    "\n",
    "**Output Format (pred.json):**\n",
    "```json\n",
    "{\n",
    "    \"partition\": \"test\",\n",
    "    \"groundtruth\": {\"bib_key\": \"arxiv_id\"},\n",
    "    \"prediction\": {\"bib_key\": [\"candidate_1\", \"candidate_2\", ...]}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
