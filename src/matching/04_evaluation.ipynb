{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d538f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d56c66",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004558c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"../../23120260\")\n",
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "\n",
    "class ReferenceMatchingModel:\n",
    "    def __init__(self):\n",
    "        self.weights, self.bias = None, 0.0\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "    \n",
    "    def load(self, path: Path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.weights = np.array(data['weights']) if data['weights'] else None\n",
    "        self.bias = data['bias']\n",
    "\n",
    "model = ReferenceMatchingModel()\n",
    "model.load(OUTPUT_DIR / 'reference_matching_model.json')\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff8b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 893 publications\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIR / 'extracted_data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "print(f\"Loaded {len(all_data)} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd14e6e",
   "metadata": {},
   "source": [
    "## 2. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf535ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(predictions: List[List[str]], ground_truth: List[str], top_k: int = 5) -> float:\n",
    "    \"\"\"Mean Reciprocal Rank\"\"\"\n",
    "    rr = []\n",
    "    for pred_list, true_id in zip(predictions, ground_truth):\n",
    "        rank = next((i + 1 for i, p in enumerate(pred_list[:top_k]) if p == true_id), 0)\n",
    "        rr.append(1.0 / rank if rank > 0 else 0.0)\n",
    "    return np.mean(rr) if rr else 0.0\n",
    "\n",
    "def calculate_precision_at_k(predictions: List[List[str]], ground_truth: List[str], k: int = 1) -> float:\n",
    "    \"\"\"Precision@K\"\"\"\n",
    "    return sum(1 for pred, true in zip(predictions, ground_truth) if true in pred[:k]) / len(predictions) if predictions else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c046f",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction (from notebook 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4c21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    STOP_WORDS = {'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not text: return \"\"\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text.lower())\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
    "        return re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s]', ' ', text)).strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_title(title: str) -> str:\n",
    "        return re.sub(r'^(on|the|a|an)\\s+', '', TextCleaner.clean_text(title))\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        return [t for t in TextCleaner.clean_text(text).split() if t not in TextCleaner.STOP_WORDS]\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_author_last_names(authors: List[str]) -> List[str]:\n",
    "        return [p[-1] for a in authors if a for p in [TextCleaner.clean_text(a).split()] if p]\n",
    "\n",
    "def extract_features(bib: Dict, ref: Dict) -> np.ndarray:\n",
    "    bib_title, ref_title = TextCleaner.clean_title(bib.get('title', '')), TextCleaner.clean_title(ref.get('title', ''))\n",
    "    bib_tokens, ref_tokens = set(TextCleaner.tokenize(bib_title)), set(TextCleaner.tokenize(ref_title))\n",
    "    \n",
    "    jaccard = len(bib_tokens & ref_tokens) / len(bib_tokens | ref_tokens) if bib_tokens and ref_tokens else 0.0\n",
    "    overlap = len(bib_tokens & ref_tokens) / min(len(bib_tokens), len(ref_tokens)) if bib_tokens and ref_tokens else 0.0\n",
    "    edit_sim = 1.0 - abs(len(bib_title) - len(ref_title)) / max(len(bib_title), len(ref_title), 1)\n",
    "    \n",
    "    bib_authors = set(TextCleaner.extract_author_last_names(bib.get('authors', [])))\n",
    "    ref_authors = set(TextCleaner.extract_author_last_names(ref.get('authors', [])))\n",
    "    author_overlap = len(bib_authors & ref_authors) / min(len(bib_authors), len(ref_authors)) if bib_authors and ref_authors else 0.0\n",
    "    first_match = 1.0 if list(bib_authors)[:1] == list(ref_authors)[:1] and bib_authors else 0.0\n",
    "    \n",
    "    bib_year, ref_year = bib.get('year', ''), ref.get('year', '')\n",
    "    year_match = 1.0 if bib_year == ref_year else 0.0\n",
    "    try: year_diff = abs(int(bib_year) - int(ref_year)) if bib_year and ref_year else 10\n",
    "    except: year_diff = 10\n",
    "    \n",
    "    bib_arxiv = (bib.get('arxiv_id') or '').replace('.', '-')\n",
    "    ref_arxiv = (ref.get('arxiv_id') or '').replace('.', '-')\n",
    "    arxiv_match = 1.0 if bib_arxiv and ref_arxiv and bib_arxiv == ref_arxiv else 0.0\n",
    "    arxiv_in_content = 1.0 if ref_arxiv.replace('-', '.') in bib.get('raw_content', '') else 0.0\n",
    "    \n",
    "    len_ratio = len(bib_title) / len(ref_title) if ref_title else 0\n",
    "    title_len_ratio = min(len_ratio, 1/len_ratio) if len_ratio > 0 else 0\n",
    "    combined = 0.4 * jaccard + 0.3 * author_overlap + 0.2 * year_match + 0.1 * first_match\n",
    "    \n",
    "    return np.array([jaccard, overlap, edit_sim, author_overlap, first_match, year_match, year_diff,\n",
    "                     arxiv_match, arxiv_in_content, len(bib_authors & ref_authors), title_len_ratio, combined])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c1b3d",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5fa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(bib: Dict, refs: Dict[str, Dict], model, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    scores = [(arxiv_id, model.predict_proba(extract_features(bib, ref).reshape(1, -1))[0]) \n",
    "              for arxiv_id, ref in refs.items()]\n",
    "    return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed78852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 677 pubs, Test: 4\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth from pred.json files\n",
    "ground_truth_lookup, partition_lookup = defaultdict(dict), {}\n",
    "\n",
    "for pub_data in all_data:\n",
    "    pred_file = DATA_DIR / pub_data['pub_id'] / \"pred.json\"\n",
    "    if pred_file.exists():\n",
    "        with open(pred_file, 'r') as f:\n",
    "            pred_data = json.load(f)\n",
    "        partition_lookup[pub_data['pub_id']] = pred_data.get('partition', 'train')\n",
    "        for bib_key, arxiv_id in pred_data.get('groundtruth', {}).items():\n",
    "            ground_truth_lookup[pub_data['pub_id']][bib_key] = arxiv_id\n",
    "\n",
    "print(f\"Ground truth: {len(ground_truth_lookup)} pubs, Test: {sum(1 for p in partition_lookup.values() if p == 'test')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b002f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 4 pubs, 24 entries\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for TEST SET\n",
    "test_predictions, test_ground_truth, test_pub_ids = [], [], []\n",
    "\n",
    "for pub_data in all_data:\n",
    "    pub_id = pub_data['pub_id']\n",
    "    if partition_lookup.get(pub_id) != 'test' or pub_id not in ground_truth_lookup:\n",
    "        continue\n",
    "    \n",
    "    for bib in pub_data['bibs']:\n",
    "        if bib['key'] not in ground_truth_lookup[pub_id]:\n",
    "            continue\n",
    "        ranked = rank_candidates(bib, pub_data['refs'], model, top_k=5)\n",
    "        test_predictions.append([arxiv_id for arxiv_id, _ in ranked])\n",
    "        test_ground_truth.append(ground_truth_lookup[pub_id][bib['key']])\n",
    "        test_pub_ids.append(pub_id)\n",
    "\n",
    "print(f\"Test: {len(set(test_pub_ids))} pubs, {len(test_predictions)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07439e66",
   "metadata": {},
   "source": [
    "## 5. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ce10de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TEST SET METRICS\n",
      "==================================================\n",
      "MRR@5: 0.6660, P@1: 0.5833, P@3: 0.6667, P@5: 0.8750\n",
      "  2411-00222: MRR@5=0.5156, P@1=0.4000\n",
      "  2411-00236: MRR@5=1.0000, P@1=1.0000\n",
      "  2411-00252: MRR@5=0.8500, P@1=0.8000\n",
      "  2411-00260: MRR@5=1.0000, P@1=1.0000\n"
     ]
    }
   ],
   "source": [
    "# MRR@5 and Precision metrics\n",
    "mrr_5 = calculate_mrr(test_predictions, test_ground_truth, top_k=5)\n",
    "p_at_1 = calculate_precision_at_k(test_predictions, test_ground_truth, k=1)\n",
    "p_at_3 = calculate_precision_at_k(test_predictions, test_ground_truth, k=3)\n",
    "p_at_5 = calculate_precision_at_k(test_predictions, test_ground_truth, k=5)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TEST SET METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MRR@5: {mrr_5:.4f}, P@1: {p_at_1:.4f}, P@3: {p_at_3:.4f}, P@5: {p_at_5:.4f}\")\n",
    "\n",
    "for pub in sorted(set(test_pub_ids)):\n",
    "    pub_preds = [p for p, pid in zip(test_predictions, test_pub_ids) if pid == pub]\n",
    "    pub_gt = [g for g, pid in zip(test_ground_truth, test_pub_ids) if pid == pub]\n",
    "    print(f\"  {pub}: MRR@5={calculate_mrr(pub_preds, pub_gt):.4f}, P@1={calculate_precision_at_k(pub_preds, pub_gt):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c01c2ea",
   "metadata": {},
   "source": [
    "## 6. Generate pred.json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc23584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_json(pub_path: Path, pub_data: Dict, gt_dict: Dict, model, partition: str = \"test\"):\n",
    "    \"\"\"Generate pred.json with top-5 predictions\"\"\"\n",
    "    pred_structure = {\"partition\": partition, \"groundtruth\": {}, \"prediction\": {}}\n",
    "    \n",
    "    for bib_key, arxiv_id in gt_dict.items():\n",
    "        pred_structure[\"groundtruth\"][bib_key] = arxiv_id\n",
    "        bib = next((b for b in pub_data['bibs'] if b['key'] == bib_key), None)\n",
    "        if bib:\n",
    "            ranked = rank_candidates(bib, pub_data['refs'], model, top_k=5)\n",
    "            pred_structure[\"prediction\"][bib_key] = [aid for aid, _ in ranked]\n",
    "        else:\n",
    "            pred_structure[\"prediction\"][bib_key] = []\n",
    "    \n",
    "    with open(pub_path / 'pred.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_structure, f, indent=2)\n",
    "    return pub_path / 'pred.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b97bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 677 pred.json files: Test=4, Valid=2, Train=671\n"
     ]
    }
   ],
   "source": [
    "# Generate pred.json for all publications with ground truth\n",
    "generated, partition_counts = 0, {'test': 0, 'valid': 0, 'train': 0}\n",
    "\n",
    "for pub_data in all_data:\n",
    "    pub_id = pub_data['pub_id']\n",
    "    if pub_id not in ground_truth_lookup:\n",
    "        continue\n",
    "    partition = partition_lookup.get(pub_id, 'train')\n",
    "    generate_pred_json(DATA_DIR / pub_id, pub_data, ground_truth_lookup[pub_id], model, partition)\n",
    "    generated += 1\n",
    "    partition_counts[partition] += 1\n",
    "\n",
    "print(f\"Generated {generated} pred.json files: Test={partition_counts['test']}, Valid={partition_counts['valid']}, Train={partition_counts['train']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d402b",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42345e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "Publications: 677 with labels\n",
      "Test entries: 24\n",
      "MRR@5: 0.6660, P@1: 0.5833\n",
      "Output: 677 pred.json files\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Publications: {len(ground_truth_lookup)} with labels\")\n",
    "print(f\"Test entries: {len(test_predictions)}\")\n",
    "print(f\"MRR@5: {mrr_5:.4f}, P@1: {p_at_1:.4f}\")\n",
    "print(f\"Output: {generated} pred.json files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cf6dc",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Complete!\n",
    "\n",
    "| Requirement | Implementation |\n",
    "|-------------|----------------|\n",
    "| 2.2.1 Data Cleaning | Lowercasing, tokenization, LaTeX cleanup |\n",
    "| 2.2.2 Labels | 5 manual + auto-labeled (regex/similarity) |\n",
    "| 2.2.3 Features | 12 features: title, author, year, arXiv |\n",
    "| 2.2.4 Data Split | Publication-level: test/valid/train |\n",
    "| 2.2.5 Evaluation | MRR@5 on test set |\n",
    "\n",
    "**pred.json format:** `{partition, groundtruth, prediction}`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
