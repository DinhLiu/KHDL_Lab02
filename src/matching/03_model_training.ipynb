{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026ac052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea50a67",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145693f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (5313316, 12)\n",
      "Pairs: 5313316\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "DATA_DIR = Path(\"../../23120260\")\n",
    "\n",
    "# Load features\n",
    "X = np.load(OUTPUT_DIR / 'features.npy')\n",
    "print(f\"Features: {X.shape}\")\n",
    "\n",
    "# Load metadata with streaming parser\n",
    "try:\n",
    "    import ijson\n",
    "    pair_metadata = list(ijson.items(open(OUTPUT_DIR / 'pair_metadata.json', 'rb'), 'item'))\n",
    "except ImportError:\n",
    "    with open(OUTPUT_DIR / 'pair_metadata.json', 'r', encoding='utf-8') as f:\n",
    "        pair_metadata = json.load(f)\n",
    "print(f\"Pairs: {len(pair_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f0555",
   "metadata": {},
   "source": [
    "## 2. Load Manual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7014ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No manual labels - using heuristic labels\n"
     ]
    }
   ],
   "source": [
    "# Load manual labels if available\n",
    "manual_labels_file = OUTPUT_DIR / 'manual_labels.json'\n",
    "label_lookup = {}\n",
    "\n",
    "if manual_labels_file.exists():\n",
    "    with open(manual_labels_file, 'r', encoding='utf-8') as f:\n",
    "        for label in json.load(f):\n",
    "            label_lookup[(label['pub_id'], label['bib_key'], label['arxiv_id'])] = label['is_match']\n",
    "    print(f\"Loaded {len(label_lookup)} manual labels\")\n",
    "else:\n",
    "    print(\"No manual labels - using heuristic labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd332274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 7382 positive, 5305934 negative\n"
     ]
    }
   ],
   "source": [
    "# Create labels (manual or auto-heuristic)\n",
    "y = np.zeros(len(pair_metadata))\n",
    "for i, pair in enumerate(pair_metadata):\n",
    "    key = (pair['pub_id'], pair['bib_key'], pair['arxiv_id'])\n",
    "    if key in label_lookup:\n",
    "        y[i] = 1.0 if label_lookup[key] else 0.0\n",
    "    elif X[i, 7] == 1.0 or X[i, 8] == 1.0:  # arxiv_match or arxiv_in_content\n",
    "        y[i] = 1.0\n",
    "    elif pair['combined_score'] > 0.8:\n",
    "        y[i] = 1.0\n",
    "\n",
    "print(f\"Labels: {int(y.sum())} positive, {int(len(y) - y.sum())} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9b917",
   "metadata": {},
   "source": [
    "## 3. Reference Matching Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94735e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceMatchingModel:\n",
    "    \"\"\"Logistic regression for reference matching\"\"\"\n",
    "    \n",
    "    FEATURE_NAMES = [\n",
    "        'title_jaccard', 'title_overlap', 'title_edit_dist',\n",
    "        'author_overlap', 'first_author_match', 'year_match', 'year_diff',\n",
    "        'arxiv_match', 'arxiv_in_content', 'num_matching_authors', \n",
    "        'title_len_ratio', 'combined_score'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = 0.0\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray, lr: float = 0.1, epochs: int = 1000, verbose: bool = True):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            pred = self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "            error = pred - y\n",
    "            self.weights -= lr * np.dot(X.T, error) / n_samples\n",
    "            self.bias -= lr * np.sum(error) / n_samples\n",
    "            \n",
    "            if verbose and epoch % 200 == 0:\n",
    "                loss = -np.mean(y * np.log(pred + 1e-10) + (1 - y) * np.log(1 - pred + 1e-10))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.weights is None:\n",
    "            self.weights = np.array([0.3, 0.2, 0.15, 0.25, 0.1, 0.15, -0.05, 1.0, 0.8, 0.1, 0.05, 0.0])\n",
    "        return self._sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "    \n",
    "    def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "    \n",
    "    def get_feature_importance(self) -> Dict[str, float]:\n",
    "        return {name: abs(w) for name, w in zip(self.FEATURE_NAMES, self.weights)} if self.weights is not None else {}\n",
    "    \n",
    "    def save(self, path: Path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({'weights': self.weights.tolist() if self.weights is not None else None,\n",
    "                       'bias': self.bias, 'feature_names': self.FEATURE_NAMES}, f, indent=2)\n",
    "    \n",
    "    def load(self, path: Path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.weights = np.array(data['weights']) if data['weights'] else None\n",
    "        self.bias = data['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d6bc5",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f447bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5310379 pairs, Valid: 1153 pairs, Test: 1784 pairs\n"
     ]
    }
   ],
   "source": [
    "# Publication-level data split\n",
    "PARTITION_ASSIGNMENTS = {\n",
    "    \"2411-00222\": \"test\", \"2411-00223\": \"valid\",\n",
    "    \"2411-00225\": \"train\", \"2411-00226\": \"train\", \"2411-00227\": \"train\",\n",
    "}\n",
    "\n",
    "# Load auto-labeled partitions\n",
    "for pub_path in DATA_DIR.iterdir():\n",
    "    if not pub_path.is_dir():\n",
    "        continue\n",
    "    pred_file = pub_path / \"pred.json\"\n",
    "    if pred_file.exists() and pub_path.name not in PARTITION_ASSIGNMENTS:\n",
    "        try:\n",
    "            with open(pred_file, 'r') as f:\n",
    "                pred_data = json.load(f)\n",
    "            PARTITION_ASSIGNMENTS[pub_path.name] = pred_data.get('partition', 'train') if isinstance(pred_data, dict) else 'train'\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Split by partition\n",
    "train_idx = np.array([i for i, p in enumerate(pair_metadata) if PARTITION_ASSIGNMENTS.get(p['pub_id'], 'train') == 'train'])\n",
    "val_idx = np.array([i for i, p in enumerate(pair_metadata) if PARTITION_ASSIGNMENTS.get(p['pub_id'], 'train') == 'valid'])\n",
    "test_idx = np.array([i for i, p in enumerate(pair_metadata) if PARTITION_ASSIGNMENTS.get(p['pub_id'], 'train') == 'test'])\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx] if len(val_idx) > 0 else np.array([]).reshape(0, X.shape[1]), y[val_idx] if len(val_idx) > 0 else np.array([])\n",
    "X_test, y_test = X[test_idx] if len(test_idx) > 0 else np.array([]).reshape(0, X.shape[1]), y[test_idx] if len(test_idx) > 0 else np.array([])\n",
    "\n",
    "print(f\"Train: {len(train_idx)} pairs, Valid: {len(val_idx)} pairs, Test: {len(test_idx)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25abdcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 200, Loss: 0.0259\n",
      "Epoch 400, Loss: 0.0173\n",
      "Epoch 600, Loss: 0.0142\n",
      "Epoch 800, Loss: 0.0127\n"
     ]
    }
   ],
   "source": [
    "model = ReferenceMatchingModel()\n",
    "model.train(X_train, y_train, lr=0.1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e09c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "  year_diff             : 1.6357\n",
      "  title_len_ratio       : 1.0467\n",
      "  year_match            : 0.6450\n",
      "  title_edit_dist       : 0.2668\n",
      "  num_matching_authors  : 0.1536\n",
      "  combined_score        : 0.0944\n",
      "  arxiv_in_content      : 0.0914\n",
      "  arxiv_match           : 0.0556\n",
      "  first_author_match    : 0.0499\n",
      "  title_jaccard         : 0.0430\n",
      "  author_overlap        : 0.0414\n",
      "  title_overlap         : 0.0277\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFeature Importance:\")\n",
    "for name, imp in sorted(model.get_feature_importance().items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name:22s}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83657274",
   "metadata": {},
   "source": [
    "## 5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e7cf136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: Acc=0.9887, P=0.0000, R=0.0000, F1=0.0000\n"
     ]
    }
   ],
   "source": [
    "# Validation metrics\n",
    "if len(X_val) > 0:\n",
    "    y_pred = model.predict(X_val)\n",
    "    tp, fp, fn = np.sum((y_pred == 1) & (y_val == 1)), np.sum((y_pred == 1) & (y_val == 0)), np.sum((y_pred == 0) & (y_val == 1))\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nValidation: Acc={np.mean(y_pred == y_val):.4f}, P={precision:.4f}, R={recall:.4f}, F1={f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca13185",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5611ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ..\\..\\output\\reference_matching_model.json\n"
     ]
    }
   ],
   "source": [
    "model.save(OUTPUT_DIR / 'reference_matching_model.json')\n",
    "print(f\"Model saved to {OUTPUT_DIR / 'reference_matching_model.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e122fe",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** `04_evaluation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
