{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ac052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea50a67",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145693f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "\n",
    "# Load features\n",
    "X = np.load(OUTPUT_DIR / 'features.npy')\n",
    "\n",
    "# Load pair metadata\n",
    "with open(OUTPUT_DIR / 'pair_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    pair_metadata = json.load(f)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Number of pairs: {len(pair_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f0555",
   "metadata": {},
   "source": [
    "## 2. Load Manual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manual labels if available\n",
    "manual_labels_file = OUTPUT_DIR / 'manual_labels.json'\n",
    "\n",
    "if manual_labels_file.exists():\n",
    "    with open(manual_labels_file, 'r', encoding='utf-8') as f:\n",
    "        manual_labels = json.load(f)\n",
    "    print(f\"Loaded {len(manual_labels)} manual labels\")\n",
    "    \n",
    "    # Convert to lookup dict\n",
    "    label_lookup = {}\n",
    "    for label in manual_labels:\n",
    "        key = (label['pub_id'], label['bib_key'], label['arxiv_id'])\n",
    "        label_lookup[key] = label['is_match']\n",
    "else:\n",
    "    print(\"No manual labels found - using heuristic labels\")\n",
    "    label_lookup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd332274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for training\n",
    "# If manual labels exist, use them; otherwise use auto-labeling heuristics\n",
    "\n",
    "y = np.zeros(len(pair_metadata))\n",
    "\n",
    "for i, pair in enumerate(pair_metadata):\n",
    "    key = (pair['pub_id'], pair['bib_key'], pair['arxiv_id'])\n",
    "    \n",
    "    if key in label_lookup:\n",
    "        # Use manual label\n",
    "        y[i] = 1.0 if label_lookup[key] else 0.0\n",
    "    else:\n",
    "        # Auto-label based on high confidence signals\n",
    "        # ArXiv match or very high combined score\n",
    "        if X[i, 7] == 1.0 or X[i, 8] == 1.0:  # arxiv_match or arxiv_in_content\n",
    "            y[i] = 1.0\n",
    "        elif pair['combined_score'] > 0.8:\n",
    "            y[i] = 1.0\n",
    "        else:\n",
    "            y[i] = 0.0\n",
    "\n",
    "print(f\"Labels: {int(y.sum())} positive, {int(len(y) - y.sum())} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9b917",
   "metadata": {},
   "source": [
    "## 3. Reference Matching Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94735e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceMatchingModel:\n",
    "    \"\"\"\n",
    "    Logistic regression model for reference matching.\n",
    "    \"\"\"\n",
    "    \n",
    "    FEATURE_NAMES = [\n",
    "        'title_jaccard', 'title_overlap', 'title_edit_dist',\n",
    "        'author_overlap', 'first_author_match',\n",
    "        'year_match', 'year_diff',\n",
    "        'arxiv_match', 'arxiv_in_content',\n",
    "        'num_matching_authors', 'title_len_ratio', 'combined_score'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = 0.0\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray, \n",
    "              learning_rate: float = 0.1, epochs: int = 1000,\n",
    "              verbose: bool = True):\n",
    "        \"\"\"Train the model using gradient descent\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            linear = np.dot(X, self.weights) + self.bias\n",
    "            predictions = self._sigmoid(linear)\n",
    "            \n",
    "            # Compute gradients\n",
    "            error = predictions - y\n",
    "            dw = np.dot(X.T, error) / n_samples\n",
    "            db = np.sum(error) / n_samples\n",
    "            \n",
    "            # Update weights\n",
    "            self.weights -= learning_rate * dw\n",
    "            self.bias -= learning_rate * db\n",
    "            \n",
    "            # Log progress\n",
    "            if verbose and epoch % 200 == 0:\n",
    "                loss = -np.mean(y * np.log(predictions + 1e-10) + \n",
    "                               (1 - y) * np.log(1 - predictions + 1e-10))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict match probabilities\"\"\"\n",
    "        if self.weights is None:\n",
    "            # Use heuristic weights if not trained\n",
    "            self.weights = np.array([\n",
    "                0.3,   # title_jaccard\n",
    "                0.2,   # title_overlap\n",
    "                0.15,  # title_edit_dist\n",
    "                0.25,  # author_overlap\n",
    "                0.1,   # first_author_match\n",
    "                0.15,  # year_match\n",
    "                -0.05, # year_diff\n",
    "                1.0,   # arxiv_match\n",
    "                0.8,   # arxiv_in_content\n",
    "                0.1,   # num_matching_authors\n",
    "                0.05,  # title_len_ratio\n",
    "                0.0    # combined_score\n",
    "            ])\n",
    "        \n",
    "        linear = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(linear)\n",
    "    \n",
    "    def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n",
    "        \"\"\"Predict binary labels\"\"\"\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "    \n",
    "    def get_feature_importance(self) -> Dict[str, float]:\n",
    "        \"\"\"Get feature importance (absolute weights)\"\"\"\n",
    "        if self.weights is None:\n",
    "            return {}\n",
    "        return {name: abs(w) for name, w in zip(self.FEATURE_NAMES, self.weights)}\n",
    "    \n",
    "    def save(self, path: Path):\n",
    "        \"\"\"Save model to file\"\"\"\n",
    "        data = {\n",
    "            'weights': self.weights.tolist() if self.weights is not None else None,\n",
    "            'bias': self.bias,\n",
    "            'feature_names': self.FEATURE_NAMES\n",
    "        }\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def load(self, path: Path):\n",
    "        \"\"\"Load model from file\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.weights = np.array(data['weights']) if data['weights'] else None\n",
    "        self.bias = data['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d6bc5",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PUBLICATION-LEVEL DATA SPLIT (Requirement 2.2.4)\n",
    "# ============================================================================\n",
    "# Test: 1 manual + 1 auto-labeled publication\n",
    "# Valid: 1 manual + 1 auto-labeled publication  \n",
    "# Train: All remaining publications\n",
    "\n",
    "DATA_DIR = Path(\"../../23120260\")\n",
    "\n",
    "# Define partition assignments\n",
    "PARTITION_ASSIGNMENTS = {\n",
    "    # Manual labels (from 00_manual_labeling.ipynb)\n",
    "    \"2411-00222\": \"test\",    # Manual test\n",
    "    \"2411-00223\": \"valid\",   # Manual valid\n",
    "    \"2411-00225\": \"train\",\n",
    "    \"2411-00226\": \"train\", \n",
    "    \"2411-00227\": \"train\",\n",
    "}\n",
    "\n",
    "# Load auto-labeled partitions from pred.json files\n",
    "for pub_path in DATA_DIR.iterdir():\n",
    "    if not pub_path.is_dir():\n",
    "        continue\n",
    "    pred_file = pub_path / \"pred.json\"\n",
    "    if pred_file.exists():\n",
    "        with open(pred_file, 'r') as f:\n",
    "            pred_data = json.load(f)\n",
    "        pub_id = pub_path.name\n",
    "        if pub_id not in PARTITION_ASSIGNMENTS:\n",
    "            # Get partition from pred.json (auto-labeled)\n",
    "            partition = pred_data.get('partition', 'train')\n",
    "            PARTITION_ASSIGNMENTS[pub_id] = partition\n",
    "\n",
    "# Create publication-to-partition lookup\n",
    "pub_to_partition = PARTITION_ASSIGNMENTS.copy()\n",
    "\n",
    "# Split indices by partition\n",
    "train_indices = []\n",
    "valid_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for i, pair in enumerate(pair_metadata):\n",
    "    pub_id = pair['pub_id']\n",
    "    partition = pub_to_partition.get(pub_id, 'train')\n",
    "    \n",
    "    if partition == 'test':\n",
    "        test_indices.append(i)\n",
    "    elif partition == 'valid':\n",
    "        valid_indices.append(i)\n",
    "    else:\n",
    "        train_indices.append(i)\n",
    "\n",
    "# Convert to arrays\n",
    "train_idx = np.array(train_indices)\n",
    "val_idx = np.array(valid_indices)\n",
    "test_idx = np.array(test_indices)\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx] if len(train_idx) > 0 else (np.array([]), np.array([]))\n",
    "X_val, y_val = X[val_idx], y[val_idx] if len(val_idx) > 0 else (np.array([]), np.array([]))\n",
    "X_test, y_test = X[test_idx], y[test_idx] if len(test_idx) > 0 else (np.array([]), np.array([]))\n",
    "\n",
    "# Summary\n",
    "test_pubs = set(pair_metadata[i]['pub_id'] for i in test_indices)\n",
    "valid_pubs = set(pair_metadata[i]['pub_id'] for i in valid_indices)\n",
    "train_pubs = set(pair_metadata[i]['pub_id'] for i in train_indices)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PUBLICATION-LEVEL DATA SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTest Set ({len(test_pubs)} publications, {len(test_idx)} pairs):\")\n",
    "for pub in sorted(test_pubs):\n",
    "    print(f\"  - {pub}\")\n",
    "\n",
    "print(f\"\\nValidation Set ({len(valid_pubs)} publications, {len(val_idx)} pairs):\")\n",
    "for pub in sorted(valid_pubs):\n",
    "    print(f\"  - {pub}\")\n",
    "\n",
    "print(f\"\\nTraining Set ({len(train_pubs)} publications, {len(train_idx)} pairs)\")\n",
    "print(f\"\\nTotal: {len(X)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = ReferenceMatchingModel()\n",
    "model.train(X_train, y_train, learning_rate=0.1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e09c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "print(\"\\n=== Feature Importance ===\")\n",
    "importance = model.get_feature_importance()\n",
    "for name, imp in sorted(importance.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name:25s}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83657274",
   "metadata": {},
   "source": [
    "## 5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7cf136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "y_proba = model.predict_proba(X_val)\n",
    "\n",
    "# Metrics\n",
    "accuracy = np.mean(y_pred == y_val)\n",
    "tp = np.sum((y_pred == 1) & (y_val == 1))\n",
    "fp = np.sum((y_pred == 1) & (y_val == 0))\n",
    "fn = np.sum((y_pred == 0) & (y_val == 1))\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"=== Validation Metrics ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca13185",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5611ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = OUTPUT_DIR / 'reference_matching_model.json'\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e122fe",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** Continue to `04_evaluation.ipynb` to evaluate the model and generate predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
