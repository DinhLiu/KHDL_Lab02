{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026ac052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from matching.common import (\n",
    "    ReferenceMatchingModel, FeatureExtractor,\n",
    "    get_labeled_publications, load_ground_truth,\n",
    "    OUTPUT_DIR, DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea50a67",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145693f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (1948340, 12)\n",
      "Pairs: 1948340\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT_DIR and DATA_DIR imported from common.py\n",
    "\n",
    "# Load features\n",
    "X = np.load(OUTPUT_DIR / 'features.npy')\n",
    "print(f\"Features: {X.shape}\")\n",
    "\n",
    "# Load metadata with streaming parser\n",
    "try:\n",
    "    import ijson\n",
    "    pair_metadata = list(ijson.items(open(OUTPUT_DIR / 'pair_metadata.json', 'rb'), 'item'))\n",
    "except ImportError:\n",
    "    with open(OUTPUT_DIR / 'pair_metadata.json', 'r', encoding='utf-8') as f:\n",
    "        pair_metadata = json.load(f)\n",
    "print(f\"Pairs: {len(pair_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f0555",
   "metadata": {},
   "source": [
    "## 2. Load Ground Truth from pred.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7014ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ground truth for 1890 publications\n",
      "Partitions: test=2, valid=2, train=1886\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth and partitions from pred.json files (auto-detected)\n",
    "ground_truth, PARTITION_ASSIGNMENTS = load_ground_truth(DATA_DIR)\n",
    "print(f\"Loaded ground truth for {len(ground_truth)} publications\")\n",
    "print(f\"Partitions: test={sum(1 for p in PARTITION_ASSIGNMENTS.values() if p == 'test')}, \"\n",
    "      f\"valid={sum(1 for p in PARTITION_ASSIGNMENTS.values() if p == 'valid')}, \"\n",
    "      f\"train={sum(1 for p in PARTITION_ASSIGNMENTS.values() if p == 'train')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd332274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 15491 positive, 1932849 negative\n"
     ]
    }
   ],
   "source": [
    "# Create labels from ground truth (pred.json) or auto-heuristic\n",
    "y = np.zeros(len(pair_metadata))\n",
    "\n",
    "for i, pair in enumerate(pair_metadata):\n",
    "    pub_id, bib_key, arxiv_id = pair['pub_id'], pair['bib_key'], pair['arxiv_id']\n",
    "    \n",
    "    # Check if this pair is in ground truth\n",
    "    if pub_id in ground_truth and bib_key in ground_truth[pub_id]:\n",
    "        y[i] = 1.0 if ground_truth[pub_id][bib_key] == arxiv_id else 0.0\n",
    "    # Fallback to heuristics for unlabeled pairs\n",
    "    elif X[i, 7] == 1.0 or X[i, 8] == 1.0:  # arxiv_match or arxiv_in_content\n",
    "        y[i] = 1.0\n",
    "    elif pair['combined_score'] > 0.8:\n",
    "        y[i] = 1.0\n",
    "\n",
    "print(f\"Labels: {int(y.sum())} positive, {int(len(y) - y.sum())} negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9b917",
   "metadata": {},
   "source": [
    "## 3. Reference Matching Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94735e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Features (12): ['title_jaccard', 'title_overlap', 'title_edit_dist', 'author_overlap', 'first_author_match', 'year_match', 'year_diff', 'arxiv_match', 'arxiv_in_content', 'num_matching_authors', 'title_len_ratio', 'combined_score']\n"
     ]
    }
   ],
   "source": [
    "# ReferenceMatchingModel is imported from common.py\n",
    "# Display model info\n",
    "print(\"Model: Logistic Regression\")\n",
    "print(f\"Features ({len(ReferenceMatchingModel.FEATURE_NAMES)}): {ReferenceMatchingModel.FEATURE_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d6bc5",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f447bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1945804 pairs, Valid: 1341 pairs, Test: 1195 pairs\n"
     ]
    }
   ],
   "source": [
    "# Publication-level data split (partitions loaded from pred.json)\n",
    "train_idx = np.array([i for i, p in enumerate(pair_metadata) if PARTITION_ASSIGNMENTS.get(p['pub_id'], 'train') == 'train'])\n",
    "val_idx = np.array([i for i, p in enumerate(pair_metadata) if PARTITION_ASSIGNMENTS.get(p['pub_id'], 'train') == 'valid'])\n",
    "test_idx = np.array([i for i, p in enumerate(pair_metadata) if PARTITION_ASSIGNMENTS.get(p['pub_id'], 'train') == 'test'])\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx] if len(val_idx) > 0 else np.array([]).reshape(0, X.shape[1]), y[val_idx] if len(val_idx) > 0 else np.array([])\n",
    "X_test, y_test = X[test_idx] if len(test_idx) > 0 else np.array([]).reshape(0, X.shape[1]), y[test_idx] if len(test_idx) > 0 else np.array([])\n",
    "\n",
    "print(f\"Train: {len(train_idx)} pairs, Valid: {len(val_idx)} pairs, Test: {len(test_idx)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25abdcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931\n",
      "Epoch 200, Loss: 0.0515\n",
      "Epoch 400, Loss: 0.0420\n",
      "Epoch 600, Loss: 0.0376\n",
      "Epoch 800, Loss: 0.0345\n"
     ]
    }
   ],
   "source": [
    "model = ReferenceMatchingModel()\n",
    "model.train(X_train, y_train, lr=0.1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e09c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "  year_diff             : 1.3833\n",
      "  title_len_ratio       : 1.1295\n",
      "  year_match            : 0.6246\n",
      "  arxiv_in_content      : 0.5167\n",
      "  title_jaccard         : 0.4265\n",
      "  num_matching_authors  : 0.4152\n",
      "  title_overlap         : 0.2979\n",
      "  arxiv_match           : 0.2701\n",
      "  first_author_match    : 0.2043\n",
      "  author_overlap        : 0.1662\n",
      "  combined_score        : 0.1160\n",
      "  title_edit_dist       : 0.0138\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFeature Importance:\")\n",
    "for name, imp in sorted(model.get_feature_importance().items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name:22s}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83657274",
   "metadata": {},
   "source": [
    "## 5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e7cf136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation: Acc=0.9925, P=1.0000, R=0.0909, F1=0.1667\n"
     ]
    }
   ],
   "source": [
    "# Validation metrics\n",
    "if len(X_val) > 0:\n",
    "    y_pred = model.predict(X_val)\n",
    "    tp, fp, fn = np.sum((y_pred == 1) & (y_val == 1)), np.sum((y_pred == 1) & (y_val == 0)), np.sum((y_pred == 0) & (y_val == 1))\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nValidation: Acc={np.mean(y_pred == y_val):.4f}, P={precision:.4f}, R={recall:.4f}, F1={f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca13185",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a5611ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to c:\\Code\\KHDL_Lab02_v2\\output\\reference_matching_model.json\n"
     ]
    }
   ],
   "source": [
    "model.save(OUTPUT_DIR / 'reference_matching_model.json')\n",
    "print(f\"Model saved to {OUTPUT_DIR / 'reference_matching_model.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e122fe",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** `04_evaluation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
