{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from matching import BibEntry, RefEntry, TextCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce79fd1",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "\n",
    "# Load extracted data from previous notebook\n",
    "with open(OUTPUT_DIR / 'extracted_data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(all_data)} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb84acf",
   "metadata": {},
   "source": [
    "## 2. Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features for reference matching.\n",
    "    \n",
    "    Problem Framing: This is a RANKING problem where for each BibTeX entry,\n",
    "    we need to rank all candidate references from references.json.\n",
    "    \"\"\"\n",
    "    \n",
    "    FEATURE_NAMES = [\n",
    "        'title_jaccard', 'title_overlap', 'title_edit_dist',\n",
    "        'author_overlap', 'first_author_match',\n",
    "        'year_match', 'year_diff',\n",
    "        'arxiv_match', 'arxiv_in_content',\n",
    "        'num_matching_authors', 'title_len_ratio', 'combined_score'\n",
    "    ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard_similarity(set1: Set[str], set2: Set[str]) -> float:\n",
    "        \"\"\"Jaccard similarity between two sets\"\"\"\n",
    "        if not set1 or not set2:\n",
    "            return 0.0\n",
    "        intersection = len(set1 & set2)\n",
    "        union = len(set1 | set2)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def token_overlap_ratio(tokens1: List[str], tokens2: List[str]) -> float:\n",
    "        \"\"\"Ratio of overlapping tokens\"\"\"\n",
    "        if not tokens1 or not tokens2:\n",
    "            return 0.0\n",
    "        set1, set2 = set(tokens1), set(tokens2)\n",
    "        overlap = len(set1 & set2)\n",
    "        return overlap / min(len(set1), len(set2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "        \"\"\"Compute Levenshtein distance between two strings\"\"\"\n",
    "        if len(s1) < len(s2):\n",
    "            return FeatureExtractor.levenshtein_distance(s2, s1)\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "        \n",
    "        prev_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            curr_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = prev_row[j + 1] + 1\n",
    "                deletions = curr_row[j] + 1\n",
    "                substitutions = prev_row[j] + (c1 != c2)\n",
    "                curr_row.append(min(insertions, deletions, substitutions))\n",
    "            prev_row = curr_row\n",
    "        \n",
    "        return prev_row[-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalized_edit_distance(s1: str, s2: str) -> float:\n",
    "        \"\"\"Normalized edit distance (0 = identical, 1 = completely different)\"\"\"\n",
    "        if not s1 and not s2:\n",
    "            return 0.0\n",
    "        if not s1 or not s2:\n",
    "            return 1.0\n",
    "        dist = FeatureExtractor.levenshtein_distance(s1, s2)\n",
    "        return dist / max(len(s1), len(s2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_features(bib: Dict, ref: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Extract features for a (BibEntry, RefEntry) pair.\n",
    "        \n",
    "        Args:\n",
    "            bib: BibEntry as dict\n",
    "            ref: RefEntry as dict\n",
    "        \n",
    "        Returns:\n",
    "            Feature dictionary\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Clean texts\n",
    "        bib_title = TextCleaner.clean_title(bib.get('title', ''))\n",
    "        ref_title = TextCleaner.clean_title(ref.get('title', ''))\n",
    "        \n",
    "        bib_title_tokens = TextCleaner.tokenize(bib_title)\n",
    "        ref_title_tokens = TextCleaner.tokenize(ref_title)\n",
    "        \n",
    "        # Feature 1: Title Jaccard Similarity\n",
    "        features['title_jaccard'] = FeatureExtractor.jaccard_similarity(\n",
    "            set(bib_title_tokens), set(ref_title_tokens)\n",
    "        )\n",
    "        \n",
    "        # Feature 2: Title Token Overlap Ratio\n",
    "        features['title_overlap'] = FeatureExtractor.token_overlap_ratio(\n",
    "            bib_title_tokens, ref_title_tokens\n",
    "        )\n",
    "        \n",
    "        # Feature 3: Normalized Edit Distance of Titles\n",
    "        features['title_edit_dist'] = 1.0 - FeatureExtractor.normalized_edit_distance(\n",
    "            bib_title, ref_title\n",
    "        )\n",
    "        \n",
    "        # Feature 4: Author Last Name Overlap\n",
    "        bib_authors = TextCleaner.extract_author_last_names(bib.get('authors', []))\n",
    "        ref_authors = TextCleaner.extract_author_last_names(ref.get('authors', []))\n",
    "        features['author_overlap'] = FeatureExtractor.token_overlap_ratio(\n",
    "            bib_authors, ref_authors\n",
    "        )\n",
    "        \n",
    "        # Feature 5: First Author Match\n",
    "        features['first_author_match'] = 1.0 if (\n",
    "            bib_authors and ref_authors and bib_authors[0] == ref_authors[0]\n",
    "        ) else 0.0\n",
    "        \n",
    "        # Feature 6: Year Match\n",
    "        bib_year = bib.get('year') or TextCleaner.extract_year(bib.get('raw_content', ''))\n",
    "        ref_year = ref.get('year', '')\n",
    "        features['year_match'] = 1.0 if bib_year == ref_year else 0.0\n",
    "        \n",
    "        # Year difference\n",
    "        try:\n",
    "            if bib_year and ref_year:\n",
    "                features['year_diff'] = abs(int(bib_year) - int(ref_year))\n",
    "            else:\n",
    "                features['year_diff'] = 10\n",
    "        except ValueError:\n",
    "            features['year_diff'] = 10\n",
    "        \n",
    "        # Feature 7: ArXiv ID Exact Match (strong signal!)\n",
    "        bib_arxiv = (bib.get('arxiv_id') or '').replace('.', '-')\n",
    "        ref_arxiv = (ref.get('arxiv_id') or '').replace('.', '-')\n",
    "        features['arxiv_match'] = 1.0 if (bib_arxiv and ref_arxiv and bib_arxiv == ref_arxiv) else 0.0\n",
    "        \n",
    "        # Feature 8: ArXiv ID in raw content\n",
    "        raw_content = bib.get('raw_content', '')\n",
    "        ref_arxiv_dot = ref_arxiv.replace('-', '.')\n",
    "        features['arxiv_in_content'] = 1.0 if ref_arxiv_dot and ref_arxiv_dot in raw_content else 0.0\n",
    "        \n",
    "        # Feature 9: Number of matching authors\n",
    "        features['num_matching_authors'] = len(set(bib_authors) & set(ref_authors))\n",
    "        \n",
    "        # Feature 10: Title length ratio\n",
    "        len_ratio = len(bib_title) / len(ref_title) if ref_title else 0\n",
    "        features['title_len_ratio'] = min(len_ratio, 1/len_ratio) if len_ratio > 0 else 0\n",
    "        \n",
    "        # Feature 11: Combined score\n",
    "        features['combined_score'] = (\n",
    "            0.4 * features['title_jaccard'] +\n",
    "            0.3 * features['author_overlap'] +\n",
    "            0.2 * features['year_match'] +\n",
    "            0.1 * features['first_author_match']\n",
    "        )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def features_to_vector(features: Dict[str, float]) -> np.ndarray:\n",
    "        \"\"\"Convert feature dict to numpy array\"\"\"\n",
    "        return np.array([features.get(name, 0.0) for name in FeatureExtractor.FEATURE_NAMES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5aa020",
   "metadata": {},
   "source": [
    "## 3. Test Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on sample data\n",
    "sample = all_data[0]\n",
    "print(f\"Publication: {sample['pub_id']}\")\n",
    "\n",
    "# Get first bib and first ref\n",
    "bib = sample['bibs'][0]\n",
    "ref_key = list(sample['refs'].keys())[0]\n",
    "ref = sample['refs'][ref_key]\n",
    "\n",
    "print(f\"\\nBib: {bib['key']}\")\n",
    "print(f\"  Title: {bib['title'][:60]}...\")\n",
    "\n",
    "print(f\"\\nRef: {ref_key}\")\n",
    "print(f\"  Title: {ref['title'][:60]}...\")\n",
    "\n",
    "# Extract features\n",
    "features = FeatureExtractor.extract_features(bib, ref)\n",
    "print(\"\\nFeatures:\")\n",
    "for name, value in features.items():\n",
    "    print(f\"  {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda72f27",
   "metadata": {},
   "source": [
    "## 4. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_pairs(pub_data: Dict, max_candidates: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate candidate pairs for a publication.\n",
    "    For each bib entry, create pairs with top candidates.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for bib in pub_data['bibs']:\n",
    "        # Extract features for all references\n",
    "        candidates = []\n",
    "        for arxiv_id, ref in pub_data['refs'].items():\n",
    "            features = FeatureExtractor.extract_features(bib, ref)\n",
    "            candidates.append({\n",
    "                'pub_id': pub_data['pub_id'],\n",
    "                'bib_key': bib['key'],\n",
    "                'arxiv_id': arxiv_id,\n",
    "                'features': features\n",
    "            })\n",
    "        \n",
    "        # Sort by combined score and take top candidates\n",
    "        candidates.sort(key=lambda x: x['features']['combined_score'], reverse=True)\n",
    "        pairs.extend(candidates[:max_candidates])\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dea897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pairs for all publications\n",
    "all_pairs = []\n",
    "\n",
    "for i, pub_data in enumerate(all_data):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processing {i}/{len(all_data)}...\")\n",
    "    \n",
    "    pairs = generate_candidate_pairs(pub_data)\n",
    "    all_pairs.extend(pairs)\n",
    "\n",
    "print(f\"\\nGenerated {len(all_pairs)} candidate pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to feature matrix\n",
    "X = np.array([FeatureExtractor.features_to_vector(p['features']) for p in all_pairs])\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Save feature data\n",
    "np.save(OUTPUT_DIR / 'features.npy', X)\n",
    "\n",
    "# Save pair metadata\n",
    "pair_metadata = [{\n",
    "    'pub_id': p['pub_id'],\n",
    "    'bib_key': p['bib_key'],\n",
    "    'arxiv_id': p['arxiv_id'],\n",
    "    'combined_score': p['features']['combined_score']\n",
    "} for p in all_pairs]\n",
    "\n",
    "with open(OUTPUT_DIR / 'pair_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(pair_metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved features and metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f245ced",
   "metadata": {},
   "source": [
    "## 5. Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ab6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "print(\"=== Feature Statistics ===\")\n",
    "for i, name in enumerate(FeatureExtractor.FEATURE_NAMES):\n",
    "    col = X[:, i]\n",
    "    print(f\"{name:25s}: mean={col.mean():.4f}, std={col.std():.4f}, min={col.min():.4f}, max={col.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a6aba",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** Continue to `03_model_training.ipynb` to train the matching model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
