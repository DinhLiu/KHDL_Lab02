{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4952194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from matching import BibEntry, RefEntry, TextCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce79fd1",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dada037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 302 publications\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "OUTPUT_DIR = Path(\"../../output\")\n",
    "\n",
    "# Load extracted data from previous notebook\n",
    "with open(OUTPUT_DIR / 'extracted_data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(all_data)} publications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb84acf",
   "metadata": {},
   "source": [
    "## 2. Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bab62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features for reference matching.\n",
    "    \n",
    "    Problem Framing: This is a RANKING problem where for each BibTeX entry,\n",
    "    we need to rank all candidate references from references.json.\n",
    "    \"\"\"\n",
    "    \n",
    "    FEATURE_NAMES = [\n",
    "        'title_jaccard', 'title_overlap', 'title_edit_dist',\n",
    "        'author_overlap', 'first_author_match',\n",
    "        'year_match', 'year_diff',\n",
    "        'arxiv_match', 'arxiv_in_content',\n",
    "        'num_matching_authors', 'title_len_ratio', 'combined_score'\n",
    "    ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard_similarity(set1: Set[str], set2: Set[str]) -> float:\n",
    "        \"\"\"Jaccard similarity between two sets\"\"\"\n",
    "        if not set1 or not set2:\n",
    "            return 0.0\n",
    "        intersection = len(set1 & set2)\n",
    "        union = len(set1 | set2)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def token_overlap_ratio(tokens1: List[str], tokens2: List[str]) -> float:\n",
    "        \"\"\"Ratio of overlapping tokens\"\"\"\n",
    "        if not tokens1 or not tokens2:\n",
    "            return 0.0\n",
    "        set1, set2 = set(tokens1), set(tokens2)\n",
    "        overlap = len(set1 & set2)\n",
    "        return overlap / min(len(set1), len(set2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "        \"\"\"Compute Levenshtein distance between two strings\"\"\"\n",
    "        if len(s1) < len(s2):\n",
    "            return FeatureExtractor.levenshtein_distance(s2, s1)\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "        \n",
    "        prev_row = range(len(s2) + 1)\n",
    "        for i, c1 in enumerate(s1):\n",
    "            curr_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = prev_row[j + 1] + 1\n",
    "                deletions = curr_row[j] + 1\n",
    "                substitutions = prev_row[j] + (c1 != c2)\n",
    "                curr_row.append(min(insertions, deletions, substitutions))\n",
    "            prev_row = curr_row\n",
    "        \n",
    "        return prev_row[-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalized_edit_distance(s1: str, s2: str) -> float:\n",
    "        \"\"\"Normalized edit distance (0 = identical, 1 = completely different)\"\"\"\n",
    "        if not s1 and not s2:\n",
    "            return 0.0\n",
    "        if not s1 or not s2:\n",
    "            return 1.0\n",
    "        dist = FeatureExtractor.levenshtein_distance(s1, s2)\n",
    "        return dist / max(len(s1), len(s2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_features(bib: Dict, ref: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Extract features for a (BibEntry, RefEntry) pair.\n",
    "        \n",
    "        Args:\n",
    "            bib: BibEntry as dict\n",
    "            ref: RefEntry as dict\n",
    "        \n",
    "        Returns:\n",
    "            Feature dictionary\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Clean texts\n",
    "        bib_title = TextCleaner.clean_title(bib.get('title', ''))\n",
    "        ref_title = TextCleaner.clean_title(ref.get('title', ''))\n",
    "        \n",
    "        bib_title_tokens = TextCleaner.tokenize(bib_title)\n",
    "        ref_title_tokens = TextCleaner.tokenize(ref_title)\n",
    "        \n",
    "        # Feature 1: Title Jaccard Similarity\n",
    "        features['title_jaccard'] = FeatureExtractor.jaccard_similarity(\n",
    "            set(bib_title_tokens), set(ref_title_tokens)\n",
    "        )\n",
    "        \n",
    "        # Feature 2: Title Token Overlap Ratio\n",
    "        features['title_overlap'] = FeatureExtractor.token_overlap_ratio(\n",
    "            bib_title_tokens, ref_title_tokens\n",
    "        )\n",
    "        \n",
    "        # Feature 3: Normalized Edit Distance of Titles\n",
    "        features['title_edit_dist'] = 1.0 - FeatureExtractor.normalized_edit_distance(\n",
    "            bib_title, ref_title\n",
    "        )\n",
    "        \n",
    "        # Feature 4: Author Last Name Overlap\n",
    "        bib_authors = TextCleaner.extract_author_last_names(bib.get('authors', []))\n",
    "        ref_authors = TextCleaner.extract_author_last_names(ref.get('authors', []))\n",
    "        features['author_overlap'] = FeatureExtractor.token_overlap_ratio(\n",
    "            bib_authors, ref_authors\n",
    "        )\n",
    "        \n",
    "        # Feature 5: First Author Match\n",
    "        features['first_author_match'] = 1.0 if (\n",
    "            bib_authors and ref_authors and bib_authors[0] == ref_authors[0]\n",
    "        ) else 0.0\n",
    "        \n",
    "        # Feature 6: Year Match\n",
    "        bib_year = bib.get('year') or TextCleaner.extract_year(bib.get('raw_content', ''))\n",
    "        ref_year = ref.get('year', '')\n",
    "        features['year_match'] = 1.0 if bib_year == ref_year else 0.0\n",
    "        \n",
    "        # Year difference\n",
    "        try:\n",
    "            if bib_year and ref_year:\n",
    "                features['year_diff'] = abs(int(bib_year) - int(ref_year))\n",
    "            else:\n",
    "                features['year_diff'] = 10\n",
    "        except ValueError:\n",
    "            features['year_diff'] = 10\n",
    "        \n",
    "        # Feature 7: ArXiv ID Exact Match (strong signal!)\n",
    "        bib_arxiv = (bib.get('arxiv_id') or '').replace('.', '-')\n",
    "        ref_arxiv = (ref.get('arxiv_id') or '').replace('.', '-')\n",
    "        features['arxiv_match'] = 1.0 if (bib_arxiv and ref_arxiv and bib_arxiv == ref_arxiv) else 0.0\n",
    "        \n",
    "        # Feature 8: ArXiv ID in raw content\n",
    "        raw_content = bib.get('raw_content', '')\n",
    "        ref_arxiv_dot = ref_arxiv.replace('-', '.')\n",
    "        features['arxiv_in_content'] = 1.0 if ref_arxiv_dot and ref_arxiv_dot in raw_content else 0.0\n",
    "        \n",
    "        # Feature 9: Number of matching authors\n",
    "        features['num_matching_authors'] = len(set(bib_authors) & set(ref_authors))\n",
    "        \n",
    "        # Feature 10: Title length ratio\n",
    "        len_ratio = len(bib_title) / len(ref_title) if ref_title else 0\n",
    "        features['title_len_ratio'] = min(len_ratio, 1/len_ratio) if len_ratio > 0 else 0\n",
    "        \n",
    "        # Feature 11: Combined score\n",
    "        features['combined_score'] = (\n",
    "            0.4 * features['title_jaccard'] +\n",
    "            0.3 * features['author_overlap'] +\n",
    "            0.2 * features['year_match'] +\n",
    "            0.1 * features['first_author_match']\n",
    "        )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def features_to_vector(features: Dict[str, float]) -> np.ndarray:\n",
    "        \"\"\"Convert feature dict to numpy array\"\"\"\n",
    "        return np.array([features.get(name, 0.0) for name in FeatureExtractor.FEATURE_NAMES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5aa020",
   "metadata": {},
   "source": [
    "## 3. Test Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e7b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication: 2411-00222\n",
      "\n",
      "Bib: ganjidoost2024protectingfeedforwardnetworksadversarial\n",
      "  Title: Protecting Feed-Forward Networks from Adversarial Attacks Us...\n",
      "\n",
      "Ref: 2006-04182\n",
      "  Title: Predictive Coding Approximates Backprop along Arbitrary Comp...\n",
      "\n",
      "Features:\n",
      "  title_jaccard: 0.1333\n",
      "  title_overlap: 0.2500\n",
      "  title_edit_dist: 0.2346\n",
      "  author_overlap: 0.0000\n",
      "  first_author_match: 0.0000\n",
      "  year_match: 0.0000\n",
      "  year_diff: 4.0000\n",
      "  arxiv_match: 0.0000\n",
      "  arxiv_in_content: 0.0000\n",
      "  num_matching_authors: 0.0000\n",
      "  title_len_ratio: 0.9136\n",
      "  combined_score: 0.0533\n"
     ]
    }
   ],
   "source": [
    "# Test on sample data\n",
    "sample = all_data[0]\n",
    "print(f\"Publication: {sample['pub_id']}\")\n",
    "\n",
    "# Get first bib and first ref\n",
    "bib = sample['bibs'][0]\n",
    "ref_key = list(sample['refs'].keys())[0]\n",
    "ref = sample['refs'][ref_key]\n",
    "\n",
    "print(f\"\\nBib: {bib['key']}\")\n",
    "print(f\"  Title: {bib['title'][:60]}...\")\n",
    "\n",
    "print(f\"\\nRef: {ref_key}\")\n",
    "print(f\"  Title: {ref['title'][:60]}...\")\n",
    "\n",
    "# Extract features\n",
    "features = FeatureExtractor.extract_features(bib, ref)\n",
    "print(\"\\nFeatures:\")\n",
    "for name, value in features.items():\n",
    "    print(f\"  {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda72f27",
   "metadata": {},
   "source": [
    "## 4. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0303189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_pairs(pub_data: Dict, max_candidates: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate candidate pairs for a publication.\n",
    "    For each bib entry, create pairs with top candidates.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for bib in pub_data['bibs']:\n",
    "        # Extract features for all references\n",
    "        candidates = []\n",
    "        for arxiv_id, ref in pub_data['refs'].items():\n",
    "            t1 = bib['title'].replace('\\n', ' ').strip()\n",
    "            t2 = ref['title'].replace('\\n', ' ').strip()\n",
    "\n",
    "            if abs(len(t1) - len(t2)) > 40:\n",
    "                continue\n",
    "\n",
    "            features = FeatureExtractor.extract_features(bib, ref)\n",
    "            candidates.append({\n",
    "                'pub_id': pub_data['pub_id'],\n",
    "                'bib_key': bib['key'],\n",
    "                'arxiv_id': arxiv_id,\n",
    "                'features': features\n",
    "            })\n",
    "        \n",
    "        # Sort by combined score and take top candidates\n",
    "        candidates.sort(key=lambda x: x['features']['combined_score'], reverse=True)\n",
    "        pairs.extend(candidates[:max_candidates])\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8dea897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 workers, batch size: 500\n",
      "Total publications: 302\n",
      "\n",
      "--- Batch 1/1 (publications 0-302) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 302/302 [00:04<00:00, 61.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch pairs: 703946, Total so far: 703946\n",
      "\n",
      "--- Saving results ---\n",
      "Feature matrix shape: (703946, 12)\n",
      "Saved features to: ..\\..\\output\\features.npy\n",
      "Saved metadata to: ..\\..\\output\\pair_metadata.json\n",
      "\n",
      "=== SUMMARY ===\n",
      "Total pairs generated: 703946\n",
      "Feature matrix shape: (703946, 12)\n"
     ]
    }
   ],
   "source": [
    "# Generate pairs using multiprocessing with batch processing to save RAM\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "# Import the worker function from the separate module\n",
    "from feature_processor import generate_candidate_pairs_worker, FeatureProcessor\n",
    "\n",
    "# Configuration\n",
    "num_workers = min(multiprocessing.cpu_count(), 8)\n",
    "batch_size = 500  # Process this many publications per batch\n",
    "num_features = len(FeatureProcessor.FEATURE_NAMES)\n",
    "\n",
    "print(f\"Using {num_workers} workers, batch size: {batch_size}\")\n",
    "print(f\"Total publications: {len(all_data)}\")\n",
    "\n",
    "# Initialize output files\n",
    "features_file = OUTPUT_DIR / 'features.npy'\n",
    "metadata_file = OUTPUT_DIR / 'pair_metadata.json'\n",
    "\n",
    "# Process in batches\n",
    "total_pairs = 0\n",
    "all_metadata = []\n",
    "all_features_list = []\n",
    "\n",
    "num_batches = (len(all_data) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(all_data))\n",
    "    batch_data = all_data[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\n--- Batch {batch_idx + 1}/{num_batches} (publications {start_idx}-{end_idx}) ---\")\n",
    "    \n",
    "    batch_pairs = []\n",
    "    \n",
    "    # Process batch with multiprocessing\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(generate_candidate_pairs_worker, pub_data) \n",
    "                   for pub_data in batch_data]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(batch_data), \n",
    "                          desc=f\"Batch {batch_idx + 1}\"):\n",
    "            try:\n",
    "                pairs = future.result()\n",
    "                batch_pairs.extend(pairs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    \n",
    "    # Extract features and metadata from batch\n",
    "    batch_features = np.array([\n",
    "        [p['features'].get(name, 0.0) for name in FeatureProcessor.FEATURE_NAMES]\n",
    "        for p in batch_pairs\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    batch_metadata = [{\n",
    "        'pub_id': p['pub_id'],\n",
    "        'bib_key': p['bib_key'],\n",
    "        'arxiv_id': p['arxiv_id'],\n",
    "        'combined_score': p['features']['combined_score']\n",
    "    } for p in batch_pairs]\n",
    "    \n",
    "    # Accumulate results\n",
    "    all_features_list.append(batch_features)\n",
    "    all_metadata.extend(batch_metadata)\n",
    "    \n",
    "    total_pairs += len(batch_pairs)\n",
    "    print(f\"  Batch pairs: {len(batch_pairs)}, Total so far: {total_pairs}\")\n",
    "    \n",
    "    # Clear batch from memory\n",
    "    del batch_pairs, batch_features, batch_metadata\n",
    "\n",
    "# Concatenate all features and save\n",
    "print(f\"\\n--- Saving results ---\")\n",
    "X = np.vstack(all_features_list)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "np.save(features_file, X)\n",
    "print(f\"Saved features to: {features_file}\")\n",
    "\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_metadata, f, indent=2)\n",
    "print(f\"Saved metadata to: {metadata_file}\")\n",
    "\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total pairs generated: {total_pairs}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f245ced",
   "metadata": {},
   "source": [
    "## 5. Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319ab6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Statistics ===\n",
      "title_jaccard            : mean=0.0651, std=0.1113, min=0.0000, max=1.0000\n",
      "title_overlap            : mean=0.1269, std=0.1634, min=0.0000, max=1.0000\n",
      "title_edit_dist          : mean=0.2492, std=0.0974, min=0.0000, max=1.0000\n",
      "author_overlap           : mean=0.0110, std=0.0758, min=0.0000, max=1.0000\n",
      "first_author_match       : mean=0.0040, std=0.0630, min=0.0000, max=1.0000\n",
      "year_match               : mean=0.1107, std=0.3138, min=0.0000, max=1.0000\n",
      "year_diff                : mean=7.7554, std=19.4024, min=0.0000, max=2018.0000\n",
      "arxiv_match              : mean=0.0014, std=0.0380, min=0.0000, max=1.0000\n",
      "arxiv_in_content         : mean=0.0028, std=0.0525, min=0.0000, max=1.0000\n",
      "num_matching_authors     : mean=0.0510, std=0.4380, min=0.0000, max=58.0000\n",
      "title_len_ratio          : mean=0.7296, std=0.1884, min=0.0000, max=1.0000\n",
      "combined_score           : mean=0.0519, std=0.0907, min=0.0000, max=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Feature statistics\n",
    "print(\"=== Feature Statistics ===\")\n",
    "for i, name in enumerate(FeatureExtractor.FEATURE_NAMES):\n",
    "    col = X[:, i]\n",
    "    print(f\"{name:25s}: mean={col.mean():.4f}, std={col.std():.4f}, min={col.min():.4f}, max={col.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a6aba",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** Continue to `03_model_training.ipynb` to train the matching model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
